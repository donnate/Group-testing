---
title: "Simulations"
output: pdf_document
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(ggplot2)
library(fitdistrplus)
library(DescTools)
library(data.table)
library(dplyr)
library(reshape2)
library(RColorBrewer)
```


# Probability Law Functions
The probability of having $K$ total positives in a pool of size $n$ is the probability of $k$ being infected in the community (probability of infection equals prevalence) and $K-k$ being infected via network transmission (probability of infection equals $\tau$).
$$
P(\sum Y_i = K) = 
     \sum_{k=1}^K \left( {n \choose k} \pi^k (1-\pi)^{n-k} {n-k \choose K- k} (1-(1-\tau)^k)^{K-k} ((1-\tau)^k)^{(n-K)} \right)
$$
Functions for computing the probability of $K$ positives in a pool of size $N$, and to sample $\pi$ and $\tau$ from prior distributions, are in the `prob_laws_429.R` script. 
```{r}
source("prob_laws_429.R")
```

# Load real Ct data and extract the distribution parameters
```{r}
# Get all tests
tests <- data.table::fread("alltests_1mar24jun_v1.csv")

# Keep only valid positive ct values for first tests
tests %>% filter(result == "positive", firsttest==TRUE,!is.na(cttarget)) %>% pull(cttarget) -> cts

# Fit Weibull Distribution
fw <- fitdist(cts, "weibull")
```

```{r}
## GENERATE UN-CORRELATED Ct DATA
# Add variable of proportion Ct value >LoD to model, as surrogate for differences in viral load distribution in different populations

# Number of replicates to generate ecdf offset values for above.
set.seed(42)
x<-5000

# Create simulated vector of ct values with same shape as that from Weibull distribution fit to ct values from real first tests
# 5000 draws from weibull with shape and scale parameters drawn from real data
ct_fake_input <- rweibull(x,fw[[1]][1],fw[[1]][2]) 

# Create matrix of desired input parameters
# Change above.lod to % samples with ct value >LoD to reflect actual population of interest. Changing "lod" itself has no effect on model output.
lod <- 35
above.lod <- seq(0.05,0.3,0.05)
translation_vector <- seq(-10,15,0.01)
mat<-matrix(ncol=3,nrow=length(translation_vector))

# Loop 1: fix LOD; for known shift, what percent of samples are above LoD (a)? 
for(v in 1:length(translation_vector)){
  i=1 
  fn<-ecdf(subset(pmax(5,ct_fake_input+translation_vector[v]),
                  (pmax(5,ct_fake_input+translation_vector[v]))<45)) ## selects adequate values within the sample
			a<-1-fn(lod) ## percent of cvalues in the distribution above the LoD 
			mat[(v-1)+i,1]<-lod
			mat[(v-1)+i,2]<-a
			mat[(v-1)+i,3]<-translation_vector[v]
}

# Shift ct values for each LoD and %above lod
mat2<-matrix(ncol=3,nrow=length(above.lod))

# Loop 2: fix LOD; for known a, how much should you shift? 
for(j in 1:length(above.lod)){
	tmp<-subset(mat,mat[,1]==lod)
	u<-tmp[which.min(abs(above.lod[j]-tmp[,2])),3]
	mat2[j,1]<-lod[i]
	mat2[j,2]<-above.lod[j]
	mat2[j,3]<-u
}
```

# Include the probit  coefficients
The probits scores are necessary to compute the viral load. Specimens with Ct beyond (greater than) the LoD are assigned a decreasing probability of detection based on a probit regression curve. Probability of detection is derived form the probit regression model. 
```{r}
# probit data input
probit_input <- read.csv("probit_zscores_cts_tissue_agnostic.csv")
probit_t<-subset(probit_input,probit_input$z_value<=1.96 & probit_input$z_value>=-1.96)
probit<-probit_t[,c(2,4,3)]
probit<-probit[order(probit$z_value,probit$ct_value),]
z_scores<-as.numeric(unlist(distinct(probit,z_value)))

# Number of replicates for model
set.seed(42)
# n <- 10000

probit.mode<-c("base","dsa.lower","dsa.upper","psa") 
probit.mode.index<-1 # 1 = no variation, 2, = LLN, 3 = ULN, 4 = probabilistic
probit.z.indices<-c(488,1,length(z_scores)) # 488 is a z score of 0 (base case) in the z index vector
dilution.vary.index<-1 # 1 = no variation, 2 = probabilistic
```

# Compute Sensitivity of Test in Pooled Samples
Sensitivity is a funciton of the Ct values of the sample, which changes as a function of the number of positives in a pool of a given size (dilution). Import the function to compute sensitivity of the PCR tet on pooled samples, for a pool of size N with 1:N positives in the pool.
```{r}
source("sensitivity_fxn_429.R")
# default inputs into sens_fxn() are
# N, B = 1000, above = 5, ct_dat = ct_fake_input

sens_dat <- data.frame(do.call(rbind, sapply(1:20, function(n) { sens_fxn(n)})))
sens_dat$pos_frac <- sens_dat$positives / sens_dat$pool.size
#probit %>% filter(z_value == 0) %>% group_by(ct_value) %>% summarise(mean = mean(probit_probability_detection))
```

Expected Ct based on the observed data (individual PCR testing)
??? Is this correct ???
```{r}
# cts is the observed data
# ct_fake_input is the simulated data from Weibull distribution, using parameters from 
# observed data
head(ct_fake_input)
ct_simp <- round(ct_fake_input, 1) # round to one decimal place (to match ct val in probit data)
ct_simp <- ct_simp[order(ct_simp)] # order by ascending ct value
freq_ct <- table(ct_simp)/length(ct_fake_input) # calculate frequency of each ct value
probit_sub <- probit[probit$ct_value %in% ct_simp,] # subset probit to ct_values that occur in our observed data
# (any values that do not occur in data have probability 0)
probit_sub <- probit_sub %>% group_by(ct_value) %>% filter(row_number() == 1) # remove duplicates (one obs per Ct value)
sum(probit_sub$probit_probability_detection * freq_ct)

freq_ct_dat <- data.frame(freq_ct)
colnames(freq_ct_dat) <- c("ct_value", "freq")
df <- merge(probit, freq_ct_dat, on = ct_value)
df <- df %>% group_by(ct_value) %>% filter(row_number() == 1)
sum(df$probit_probability_detection * freq_ct_dat$freq)
```

Plot Sensitivity as a function of number infected per pool
```{r}
# data frame of sensitivity as function of number positive per pool
ggplot(sens_dat, aes(x=pool.size, y=mean, group=as.factor(positives))) +
  geom_line(aes(color=as.factor(positives))) + 
  theme(legend.position="bottom") + 
  guides(fill=guide_legend(title="Number Positive")) + 
  geom_hline(yintercept = sens_dat$mean[sens_dat$pool.size==1]) + 
  xlab("Pool size") + ylab("Sensitivity") 

ggplot(sens_dat, aes(x=pos_frac, y=mean, group=as.factor(pool.size))) +
  geom_line(aes(color=as.factor(pool.size))) + 
  guides(color=guide_legend(title="Pool Size")) + 
  geom_hline(yintercept = sens_dat$mean[sens_dat$pool.size==1]) + 
  theme(legend.position="bottom") + 
  xlab("Number Positive / Pool size") + ylab("Sensitivity")   

```
Plot sensitivity as a function of Ct values
```{r}
ggplot(sens_dat, aes(x=ct_avg, y=mean, group=as.factor(pool.size))) +
  geom_line(aes(color=as.factor(pool.size))) + 
  guides(color=guide_legend(title="Pool Size")) + 
  geom_hline(yintercept = sens_dat$mean[sens_dat$pool.size==1]) + 
  theme(legend.position="bottom") + 
  xlab("Average Ct Value") + ylab("Sensitivity") + 
  ggtitle("Sensitivity as a function of Average Ct Value")
```


Plot probit probability of detection as a function of Ct values
```{r}
# Probit probability of detectin as fxn of Ct values
ggplot(probit %>% group_by(ct_value) %>% 
         summarize(probit_probability_detection = mean(probit_probability_detection)), 
       aes(x=ct_value, y=probit_probability_detection)) + 
  geom_point()
```

```{r}
it =1
pool.max<-20 # maximum pool size
B <- 100 # number of simulations

for (N in 1:pool.max){
  for (prev in c(0.005, 0.01, 0.05, 0.1, 0.15)){
    for(tau in seq(0, 0.6, by = 0.1)){
      print(c(prev,tau, N))
      a = list(proba_laws(N, prev, tau, 
                          tau_graph_effect=NULL,
                          tau_subject_effect=NULL, 
                          prev_graph_effect=NULL,
                          prev_subject_effect=NULL,
                          null_mod = NULL,
                          B=B),
               proba_laws(N, prev, tau, 
                          tau_graph_effect=tau_graph_effect,
                          tau_subject_effect=NULL, 
                          prev_graph_effect=NULL,
                          prev_subject_effect=NULL,
                          null_mod = NULL,
                          B=B),
               proba_laws(N, prev, tau, 
                          tau_graph_effect=NULL,
                          tau_subject_effect=NULL, 
                          prev_graph_effect=prev_graph_effect,
                          prev_subject_effect=NULL,
                          null_mod = NULL,
                          B=B),
               proba_laws(N, prev, tau, 
                          tau_graph_effect=tau_graph_effect,
                          tau_subject_effect=NULL, 
                          prev_graph_effect=prev_graph_effect,
                          prev_subject_effect=NULL,
                          null_mod = NULL,
                          B=B), 
               proba_laws(N, prev, tau, 
                          tau_graph_effect=NULL,
                          tau_subject_effect=NULL, 
                          prev_graph_effect=NULL,
                          prev_subject_effect=NULL,
                          null_mod = TRUE,
                          B=B)
      )
      
      names = c("Fixed", "Tau Graph Effect","Pi Graph Effect","All Graph Effect", "Null Model")
      for (n in 1:5){
        sens <- data.frame(sens_fxn(N)) # prob(test positive | sum(Y_i) = k)
        sens <- sens$mean 
        sens_indiv <- data.frame(sens_fxn(1))$mean
        prob_pos <- sapply(1:B, function(b){sum(a[[n]][2:(N+1), b] * sens)}) 
        # a[[n]][2:(N+1), b] is the vector of probablities (for the bth simulation)
        # of having 1:N positives in a pool of size N
        if (it ==1){
          res = data.frame(
            sensit = sapply(1:B, function(b){
              sum(a[[n]][2:(N+1), b] * sens)/sum(a[[n]][2:(N+1), b])}), #sensitivity
            
            ppa = sapply(1:B, function(b){
              (sum(a[[n]][2:(N+1), b] * sens)/sum(a[[n]][2:(N+1), b]))/sens_indiv}),
            
            num_tests = (1/N) + prob_pos,
            #num_tests = (1/N) + sapply(1:B, function(b){sum(a[[n]][2:(N+1), b])}) +,
            
            caught_cases = sapply(1:B, function(b){
              (sum(c(1:N)*a[[n]][2:(N+1), b] * sens)/sum(a[[n]][2:(N+1), b]))})/((1/N) + prob_pos),
            
            missed_cases_persample = sapply(1:B, function(b){ # missed cases per sample
              (sum(c(1:N)*a[[n]][2:(N+1), b] * (1-sens)))})/((1/N) + prob_pos),
            
            type = names[n], 
            pool_size = N, 
            prev = prev, 
            tau = tau)
        }else{
          res = rbind(res,
                      data.frame(sensit = sapply(1:B, function(b){
                        sum(a[[n]][2:(N+1), b] * sens)/sum(a[[n]][2:(N+1), b])}),
                        
                        ppa = sapply(1:B, function(b){
                          (sum(a[[n]][2:(N+1), b] * sens)/sum(a[[n]][2:(N+1), b]))/sens_indiv}),
                        
                        num_tests = (1/N) + prob_pos,
                        
                        caught_cases = sapply(1:B, function(b){
                          (sum(c(1:N)*a[[n]][2:(N+1), b] * sens)/sum(a[[n]][2:(N+1), b]))})/((1/N) + prob_pos),
                        
                        missed_cases_persample = sapply(1:B, function(b){ # missed cases per sample
                          (sum(c(1:N)*a[[n]][2:(N+1), b] * (1-sens)))})/((1/N) + prob_pos),
                        type = names[n], 
                        pool_size = N, 
                        prev = prev, 
                        tau = tau))
        }
        it = it + 1
      }
    }
  } 
}

#write.csv(res, "sim_april29.csv")
# write.csv(res, "sim_march31.csv") # sampling tau from uniform distribution

# write.csv(res, "sim_march27.csv")
# write.csv(res, "sim_march29.csv")
```


```{r}
res <- read.csv("sim_april29.csv")
```

Calculate 95 percent (emperical) confidence intervals
```{r}
conf_int <- res %>% 
  group_by(type, pool_size, tau, prev) %>% 
  summarise(sd_sens= sd(sensit), 
            sd_ppa = sd(ppa), 
            sd_tests = sd(num_tests), 
            sd_missed = sd(missed_cases_persample),
            mean_sens= mean(sensit), 
            mean_ppa = mean(ppa), 
            mean_tests = mean(num_tests), 
            mean_missed = mean(missed_cases_persample), 
            sens_q025 = quantile(sensit, probs=0.025), 
            sens_q975 = quantile(sensit, probs = 0.975), 
            ppa_q025 = quantile(ppa, probs=0.025), 
            ppa_q975 = quantile(ppa, probs = 0.975), 
            tests_q025 = quantile(num_tests, probs=0.025), 
            tests_q975 = quantile(num_tests, probs = 0.975), 
            missed_q025 = quantile(missed_cases_persample, probs=0.025), 
            missed_q975 = quantile(missed_cases_persample, probs = 0.975))
```
## Debugging

Just generate probabilities
```{r}
# inputs for debugging/testing purposes
N <- 5
prev <- .1
tau <- .2
B <- 10
```

```{r}
it=1
for (N in 1:pool.max){
  for (prev in c(0.005, 0.01, 0.05, 0.1, 0.15)){
    # 0.001, 0.001, 0.005, 0.01, 0.02, 0.03, 0.05, 0.07, 0.1, 0.12, 0.15, 0.17, 0.2, 0.25, 0.3
    for(tau in seq(0, 0.6, by = 0.1)){
      # seq(from=0.00, to=0.7, by=0.005)
      print(c(prev,tau, N))
      a = list(proba_laws(N, prev, tau, 
                          tau_graph_effect=NULL,
                          tau_subject_effect=NULL, 
                          prev_graph_effect=NULL,
                          prev_subject_effect=NULL,
                          null_mod = NULL,
                          B=10),
               proba_laws(N, prev, tau, 
                          tau_graph_effect=tau_graph_effect,
                          tau_subject_effect=NULL, 
                          prev_graph_effect=NULL,
                          prev_subject_effect=NULL,
                          null_mod = NULL,
                          B=10),
               proba_laws(N, prev, tau, 
                          tau_graph_effect=NULL,
                          tau_subject_effect=NULL, 
                          prev_graph_effect=prev_graph_effect,
                          prev_subject_effect=NULL,
                          null_mod = NULL,
                          B=10),
               proba_laws(N, prev, tau, 
                          tau_graph_effect=tau_graph_effect,
                          tau_subject_effect=NULL, 
                          prev_graph_effect=prev_graph_effect,
                          prev_subject_effect=NULL,
                          null_mod = NULL,
                          B=10), 
               proba_laws(N, prev, tau, 
                          tau_graph_effect=NULL,
                          tau_subject_effect=NULL, 
                          prev_graph_effect=NULL,
                          prev_subject_effect=NULL,
                          null_mod = TRUE,
                          B=10)
      )
      
      names = c("Fixed", "Tau Graph Effect","Pi Graph Effect","All Graph Effect", "Null Model")
      for (n in 1:5){
        prob_dat = data.frame(
            prob = t(sapply(1:B, function(b){a[[n]][,b]})),
            type = names[n], 
            pool_size = N, 
            prev = prev, 
            tau = tau) 
        if (it ==1){
          prob_dat_long <- reshape2::melt(prob_dat,
                                        # ID variables - all the variables to keep but not split apart on
                                        id.vars=c("type", "pool_size", "prev", "tau"),
                                        # The source columns are all remaining columns
                                        # Name of the destination column that will identify the original
                                        # column that the measurement came from
                                        variable.name="num_pos",
                                        value.name="prob_pos")
          prob_dat_long$num_pos <- rep(0:N, each = B)
          check <- data.frame(prob_sum = (1/B)*sum(prob_dat_long[,"prob_pos"]), 
                              good = ifelse((1/B)*sum(prob_dat_long[,"prob_pos"]) == 1, 1, 0),
                              type = names[n], 
                              pool_size = N, 
                              prev = prev, 
                              tau = tau)
        }else{
          temp <- reshape2::melt(prob_dat,
                                 # ID variables - all the variables to keep but not split apart on
                                 id.vars=c("type", "pool_size", "prev", "tau"),
                                 # The source columns are all remaining columns
                                 # Name of the destination column that will identify the original
                                 # column that the measurement came from
                                 variable.name="num_pos",
                                 value.name="prob_pos")
          temp$num_pos <- rep(0:N, each = B)
          check_temp <- data.frame(prob_sum = (1/B)*sum(temp[,"prob_pos"]),
                                   good = ifelse((1/B)*sum(temp[,"prob_pos"]) == 1, 1, 0), 
                                   type = names[n], 
                                   pool_size = N, 
                                   prev = prev, 
                                   tau = tau)
          check <- rbind(check, check_temp)
          prob_dat_long = rbind(prob_dat_long, temp)
          
        }
        prob_dat <- NULL
        it = it + 1
      }
    }
  } 
}

#t(sapply(1:B, function(b){a[[1]][,b]}))
```


```{r}
table(check$good) #61 probs don't sum to 1
bad <- check[which(check$good == 0),]
# apparently the sum is 1 so these are actually ok?
```



TODO: Plot probabilities 
plot cumulative distribution functions
plot for a specific k
```{r}
colourCount <- length(unique(prob_dat_long$num_pos))
myColors <- colorRampPalette(brewer.pal(9, "Set1"))(colourCount)
prev.labs <- c("Prevalence= 0.005", "0.01", "0.05", "0.1", "0.15")
names(prev.labs) <- c("0.005", "0.01", "0.05", "0.1", "0.15")
tau.labs <- c("Tau = 0.0", "0.1", "0.2", "0.3", "0.4", "0.5", "0.6")
names(tau.labs) <- c("0", "0.1", "0.2", "0.3", "0.4", "0.5", "0.6")

ggplot(prob_dat_long, mapping = aes(x=pool_size, y=prob_pos, fill=as.factor(num_pos))) +
  geom_bar(position = "stack", stat = "identity") +
  facet_grid(tau ~ prev, labeller = labeller(tau = tau.labs, prevalence = prev.labs)) +
  theme_bw() +
  scale_fill_manual(values = myColors, name="Number of\ntests positive") +
  xlab("Pool size") + ylab("Probability") +
  ggtitle("Probability of K positives given pool size and population prevalence") +
  scale_x_continuous(limits=c(1,20), breaks=seq(1,20,4)) + scale_y_continuous(limits=c(0,1.01),breaks=seq(0,2,.2))


ggplot(prob_dat_long, mapping = aes(x=prob_pos, fill=as.factor(num_pos))) +
  geom_histogram() + 
  facet_wrap(~pool_size)
```




```{r}
res %>%
  group_by(tau, prev, pool_size) %>%
  summarise(n())
```


# Plotting





```{r}
# Separate plots for each Tau, line color is Type
tau_val <- seq(0, .6, .1)

sens_plots <- lapply(tau_val, function(i){
  ggplot(conf_int[conf_int$tau == i,], aes(x=pool_size, y= mean_sens, color=as.factor(type))) +
  geom_line() +
  facet_wrap(~ prev) +
  geom_ribbon(aes(x=pool_size, ymin = sens_q025, ymax = sens_q975, 
                  fill = as.factor(type)), show.legend = F, alpha = 0.3)+
  theme_bw() + 
  theme(axis.title.x = element_blank())+
  # scale_color_brewer(palette="RdYlBu", name="Prevalence") + 
  xlab("Pool size") + 
  ylab("Sensitivity") +
  #scale_y_continuous(limits=c(0.5, 1)) +
  ggtitle(paste("Tau =", i)) +
  #scale_x_continuous(limits=c(1,10), breaks=seq(0,10,2)) +
  guides(color=guide_legend(title="Correlation")) 
})

library(ggpubr)

pdf("sensitivity_figa.pdf",width=8, height=5)  
do.call(ggarrange, c(sens_plots[1:4], ncol = 2, nrow = 2, common.legend=TRUE, legend = "bottom"))
dev.off()

pdf("sensitivity_figb.pdf",width=8, height=5)  
do.call(ggarrange, c(sens_plots[5:7], ncol = 2, nrow = 2, common.legend=TRUE, legend = "bottom"))
dev.off()

# jitter the position so they don't overlap exactly
# compare to the null: binomial (non correlation, independence)
# to compute CI: empirical quantiles 97.5 and 2.5 from the simulation
# fix y-limits to be fro 0.5 to 1
# fix specific value of tau and prev
```
```{r}
types <- factor(c("Null Model", "All Graph Effect", "Fixed", "Pi Graph Effect", "Tau Graph Effect"))
types <- factor(types, levels = c("Null Model", "All Graph Effect", "Fixed", "Pi Graph Effect", "Tau Graph Effect"))
levels(types)
lapply(types[2:5], function(i){
  ggplot(conf_int %>% filter( type == "Null Model" | type == i), 
       aes(x=pool_size, y= mean_sens, color=as.factor(tau))) +
  geom_line(aes(linetype = type)) +
  facet_wrap(~ prev) +
  geom_ribbon(data = . %>% filter(type==i), aes(x=pool_size, ymin = sens_q025, ymax = sens_q975, 
                  fill = as.factor(tau)), show.legend = F, alpha = 0.2, 
              colour = NA)+
  scale_linetype_manual(values = c("dashed", "solid")) +
  theme_bw() + 
  #theme(axis.title.x = element_blank())+
  xlab("Pool size") + ylab("Sensitivity") +
  ggtitle(i) +
  guides(color=guide_legend(title="Tau"), linetype=guide_legend(title="Correlation")) 
})
```


```{r}
ppa_plots <- lapply(tau_val, function(i){
  ggplot(conf_int[conf_int$tau == i,], aes(x=pool_size, y= mean_ppa, color=as.factor(type))) +
    geom_line() +
    #geom_point()+
    facet_wrap(~ prev) +
    geom_ribbon(aes(x=pool_size, ymin = ppa_ql, ymax = ppa_qu, fill = as.factor(type)), show.legend = F, alpha = 0.3)+
    theme_bw() + 
    theme(axis.title.x = element_blank())+
    xlab("Pool size") + 
    ylab("PPA") +
    ggtitle(paste("Tau =", i)) +
    guides(color=guide_legend(title="Correlation")) 
})

pdf("ppa_figa.pdf",width=8, height=5)  
do.call(ggarrange, c(ppa_plots[1:4], ncol = 2, nrow = 2, common.legend=TRUE, legend = "bottom"))
dev.off()

pdf("ppa_figb.pdf",width=8, height=5)  
do.call(ggarrange, c(ppa_plots[5:7], ncol = 2, nrow = 2, common.legend=TRUE, legend = "bottom"))
dev.off()
```

```{r}
tests_plots <- lapply(tau_val, function(i){
  ggplot(conf_int[conf_int$tau == i,], aes(x=pool_size, y= mean_tests, color=as.factor(type))) +
    geom_line() +
    #geom_point()+
    facet_wrap(~ prev) +
    geom_ribbon(aes(x=pool_size, ymin = num_tests_ql, ymax = num_tests_qu, fill = as.factor(type)), show.legend = F, alpha = 0.3)+
    theme_bw() + 
    theme(axis.title.x = element_blank())+
    xlab("Pool size") + 
    ylab("Expected Number Tests / Sample") +
    ggtitle(paste("Tau =", i)) +
    guides(color=guide_legend(title="Correlation")) 
})
pdf("tests_figa.pdf",width=8, height=5)  
do.call(ggarrange, c(tests_plots[1:4], ncol = 2, nrow = 2, common.legend=TRUE, legend = "bottom"))
dev.off()

pdf("tests_figb.pdf",width=8, height=5)  
do.call(ggarrange, c(tests_plots[5:7], ncol = 2, nrow = 2, common.legend=TRUE, legend = "bottom"))
dev.off()
```
```{r}
missed_plots <- lapply(tau_val, function(i){
  ggplot(conf_int[conf_int$tau == i,], aes(x=pool_size, y= mean_missed, color=as.factor(type))) +
    geom_line() +
    #geom_point()+
    facet_wrap(~ prev) +
    #geom_errorbar(aes(x=pool_size, ymin=ci_lower, ymax=ci_upper), width=.3) +
    geom_ribbon(aes(x=pool_size, ymin = missed_cases_ql, ymax = missed_cases_qu, fill = as.factor(type)), show.legend = F, alpha = 0.3)+
    theme_bw() + 
    theme(axis.title.x = element_blank())+
    # scale_color_brewer(palette="RdYlBu", name="Prevalence") + 
    xlab("Pool size") + 
    ylab("Missed Cases / Sample") +
    ggtitle(paste("Tau =", i)) +
    guides(color=guide_legend(title="Correlation")) 
})
pdf("missed_figa.pdf",width=8, height=5)  
do.call(ggarrange, c(missed_plots[1:4], ncol = 2, nrow = 2, common.legend=TRUE, legend = "bottom"))
dev.off()

pdf("missed_figb.pdf",width=8, height=5)  
do.call(ggarrange, c(missed_plots[5:7], ncol = 2, nrow = 2, common.legend=TRUE, legend = "bottom"))
dev.off()
```
```{r}
pdf("all_tau_3.pdf", width = 8, height = 5)
ggarrange(sens_plots[[4]], ppa_plots[[4]], tests_plots[[4]], missed_plots[[4]], 
          nrow = 2, ncol = 2, common.legend = TRUE, legend = "bottom")
dev.off()
pdf("all_tau_0.pdf", width = 8, height = 5)
ggarrange(sens_plots[[1]], ppa_plots[[1]], tests_plots[[1]], missed_plots[[1]], 
          nrow = 2, ncol = 2, common.legend = TRUE, legend = "bottom")
dev.off()
```

```{r}
# table to nummerically summarize some of the graphical results
null_res <- res %>%
  filter(tau == 0 & type == "Fixed") 
alt_res <- res %>%
  filter(tau == "0.3" & type == "All Graph Effect") 


summary_table <- allci %>%
  mutate(ppa_diff=ppa - ppa_null, 
         test_diff=tests.per.sample - tests.per.sample_null, 
         ppa_pct_increase = ((ppa-ppa_null)/ppa_null)*100, 
         test_pct_decrease = ((tests.per.sample - tests.per.sample_null)/tests.per.sample_null)*100) %>%
  filter(pool == 4 | pool == 18) %>%
  filter(prevalence == 0.001 | prevalence == 0.1) %>%
  dplyr::select(pool, prevalence, tau, 
                ppa_null, ppa_pct_increase,
                tests.per.sample_null, test_pct_decrease,
                ppa_diff, test_diff)

write.csv(summary_table, "summary_table_ppa.csv")
```



```{r}
ggplot() +
  geom_point(res, mapping = aes(x=pool_size, y= sensit, color=as.factor(type))) +
  facet_grid(tau ~ prev) +
  theme_bw() + 
  theme(axis.title.x = element_blank())+
  scale_color_brewer(palette="RdYlBu", name="Prevalence") + 
  xlab("Pool size") + 
  ylab("Sensitivity") +
  # ggtitle("Correlated vs. IID Samples") +
  scale_x_continuous(limits=c(1,20), breaks=seq(0,20,2)) + 
  guides(color=guide_legend(title="Correlation")) 

# use geom_ribbon()
```

```{r}
ggplot() +
  geom_point(res, mapping = aes(x=pool_size, y= num_tests, color=as.factor(type))) +
  facet_grid(tau ~ prev) +
  theme_bw() + 
  theme(axis.title.x = element_blank())+
  scale_color_brewer(palette="RdYlBu", name="Prevalence") + 
  xlab("Pool size") + 
  ylab("Expected Number Tests / Sample") +
  # ggtitle("Correlated vs. IID Samples") +
  scale_x_continuous(limits=c(1,20), breaks=seq(0,20,2)) + 
  guides(color=guide_legend(title="Correlation")) 
```








---
title: "Correlated Data Experiments"
author: "Claire Donnat"
date: "11/4/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(ggplot2)
library(fitdistrplus)
library(DescTools)
library(data.table)
```

## Load the real data and extract the distribution parameters

```{r loading}
# Get all tests
tests <- data.table::fread("alltests_1mar24jun_v1.csv")

# Keep only valid positive ct values for first tests
tests %>% filter(result == "positive", firsttest==TRUE,!is.na(cttarget)) %>% pull(cttarget) -> cts

# Fit distribution
# Plot distribution and skew/kurtosis of ct values of real first tests
plotdist(cts, histo = TRUE, demp = TRUE, breaks=18)
descdist(cts, boot = 1000)

# Fit distributions
fw <- fitdist(cts, "weibull")
fno <- fitdist(cts, "norm")
fg <- fitdist(cts, "gamma")
fln <- fitdist(cts, "lnorm")

# Plot fit distributions and generate goodness-of-fit statistics
par(mfrow = c(2, 2))
plot.legend <- c("weibull","normal","gamma", "lnorm")
denscomp(list(fw,fno,fg,fln), legendtext = plot.legend,xlim=c(0,50))
qqcomp(list(fw,fno,fg,fln), legendtext = plot.legend)
cdfcomp(list(fw,fno,fg,fln), legendtext = plot.legend)
ppcomp(list(fw,fno,fg,fln), legendtext = plot.legend)
gofstat(list(fw,fno,fg,fln))

```

Weibull has best fit. Extract fit parameters. We can thus assume that the Ct values are sampled from a Weibull distribution with shape 4.55 and scale 29.86.
```{r get_best_fit}
summary(fw)
shape_w = summary(fw)[[1]][1]
scale_w = summary(fw)[[1]][2]
```


The viral load distribution changes from population to population. As such, we need to take this variability into account: we consider as a surrogate for this variability the proportion of variables that are above the Limit of Detection.

To achieve this, we simply jiggle the distribution:
- Step 1: we sample 50,000 samples from our fitted weibul
- Step 2: for a fixed lod = 35, we create a dictionary between the jiggling and the quantile associated to 35.
- Step 3: we create a second dictionary mapping the percentage above the threshold and the correct jiggling.

```{r}
## UN-CORRELATED DATA
# Add variable of proportion Ct value >LoD to model, as surrogate for differences in viral load distribution in different populations

# Number of replicates to generate ecdf offset values for above.
set.seed(42)
# x<-50000
x<-5000

# Determine ct value quantiles for real first tests vs. fitted Weibull distribution
quantile(cts,c(0.5,0.25,0.75,0.95,0.99))
quantile(fw,c(0.5,0.25,0.75,0.95,0.99))

# Create simulated vector of ct values with same shape as that from Weibull distribution fit to ct values from real first tests
# 5000 draws from weibull with shape and scale parameters drawn from real data
ct_fake_input <- rweibull(x,fw[[1]][1],fw[[1]][2]) 

# Create matrix of desired input parameters
# Change above.lod to % samples with ct value >LoD to reflect actual population of interest. Changing "lod" itself has no effect on model output.
lod <- 35
above.lod <- seq(0.05,0.3,0.05)
translation_vector <- seq(-10,15,0.01)
mat<-matrix(ncol=3,nrow=length(translation_vector))

# Loop 1: fix LOD; for known shift, what percent of samples are above LOD  (a)? 
for(v in 1:length(translation_vector)){
  i=1 
  fn<-ecdf(subset(pmax(5,ct_fake_input+translation_vector[v]),
                  (pmax(5,ct_fake_input+translation_vector[v]))<45)) ### selects adequate values within the sample
			a<-1-fn(lod) ###percentage of cvalues in the distribution above the LOD 
			mat[(v-1)+i,1]<-lod
			mat[(v-1)+i,2]<-a
			mat[(v-1)+i,3]<-translation_vector[v]
}

# Shift ct values for each lod and %above lod
mat2<-matrix(ncol=3,nrow=length(above.lod))

# Loop 2: fix LOD; for known a, how much should you shift? 
for(j in 1:length(above.lod)){
	tmp<-subset(mat,mat[,1]==lod)
	u<-tmp[which.min(abs(above.lod[j]-tmp[,2])),3]
	mat2[j,1]<-lod[i]
	mat2[j,2]<-above.lod[j]
	mat2[j,3]<-u
}
```


```{r}
## CORRELATED DATA
source("generating_correlated_data.R") # contains fxn create_correlated_Cts

# Create simulated vector of ct values with same shape as that from Weibull distribution fit to ct values from real first tests
# ct_fake_input_corr <- rweibull(x,fw[[1]][1],fw[[1]][2]) # CHANGE THIS for correlated data (see below)

###  Create fake correlated Cts
G <-  4 # group size
N <-  x
# Sigma <- create_correlated_Cts(G)

# creating a single group of correlated data from Weibull dist
ct_fake_input_corr <- data.frame("Ct" = create_correlated_Cts(type="block", N=G, rho=0.8),
                            "Z" = rep(1,G))

# creating 50000 data points, in correlated groups of size G
# 50,000 data points total, groups of size 4 (12,500 groups)
for (i in 2:(N/G)){ # N%/%G? 
  ct_fake_input_corr <- rbind(ct_fake_input_corr,
                         data.frame("Ct" =create_correlated_Cts(type="block", N=G, rho=0.8),
                                     "Z" = rep(i,G))
  )
}

```

## Include the probit  coefficients

The probits scores are necessary to compute the viral load.


```{r probit_model}
## NAIVE Loop - sampling data uniformly at random
# probit data input
probit_input <- read.csv("probit_zscores_cts_tissue_agnostic.csv")
probit_t<-subset(probit_input,probit_input$z_value<=1.96 & probit_input$z_value>=-1.96)
probit<-probit_t[,c(2,4,3)]
probit<-probit[order(probit$z_value,probit$ct_value),]
z_scores<-as.numeric(unlist(distinct(probit,z_value)))

# Number of replicates for model
set.seed(42)
n <- 10000
pool.max<-20
probit.mode<-c("base","dsa.lower","dsa.upper","psa") 
probit.mode.index<-1 # 1 = no variation, 2, = LLN, 3 = ULN, 4 = probabilistic
probit.z.indices<-c(488,1,length(z_scores)) # 488 is a z score of 0 (base case) in the z index vector
dilution.vary.index<-1 # 1 = no variation, 2 = probabilistic

# Model loop, vary % above each LOD (simplified from original code)
experiment_loop <- function(above, ct_dat){
  ct_set <- pmax(5,subset(ct_dat + mat2[above,3], ct_dat + mat2[above,3]<45)) 
  # what is this line doing? (specifically, the <45 and pmax(5,))
  # what I think it is doing: 
  # (1) generate additional set of C_t values with 5-30% of values above each LoD; subset to only valid Ct values (<45)
  # (2) ct_set is the parallel maximum of the C_t values from (1) vs. 5 (why 5??)
  threshold.ct <- c(sample(ct_set, n, replace=T)) # why do we sample ct_set with replacement? doesn't this create correlation?
  
  rbindlist(lapply(1:pool.max, function (p) { # pool size
    rbindlist(lapply(c(0.001,0.01,0.03,0.05,0.1,0.15), function(prevalence) { # proportion positive tests 
      rbindlist(lapply(0:p, function(positives) { # number of positives
        
        if (positives == 0) {
          data.frame(limit=lod, pool=p, pos=positives, prevalence=prevalence, 
                     above.llod = above.lod[above], 
                     concentration=0, probability = dbinom(positives, p, prevalence), random=0, z.index=0,
                     call.each.conc=FALSE, tests=1, tn=1,tp=0,fn=0,fp=0)
        } 
        else {
          dat <- matrix(sample(threshold.ct, positives * n, replace=T), nrow=positives) 
          # sample data uniformly at random 
          # n samples of positives, rearrange into a matrix of positive rows, n columns
          
          each.conc = -log2(colSums(2^-dat)/p)+ifelse(dilution.vary.index==1,0,rnorm(mean=0,sd=1.1,n=ncol(dat))) 
          # sd of 1.1 reflects confidence interval for deviation from perfect log2 dilution in assays
          
          data.frame(
            limit=lod,
            pool=p,
            pos=positives,
            prevalence=prevalence,
            above.llod=above.lod[above],
            concentration=each.conc,
            probability = dbinom(positives, p, prevalence)/n,
            random=sample(n,n,replace = TRUE)/n, #
            z.index=ifelse(probit.mode.index<4,probit.z.indices[probit.mode.index],
                           sample(1:length(z_scores),n,replace=T))) %>%
            mutate(
              call.each.conc=probit[1+(z.index-1)*571+each.conc*10-(lod-35.9)*10,2]>random,
              tests=1 + p * (call.each.conc), # number of tests done (number positive pools + pool size)
              tn=0,
              tp=1 * (call.each.conc),
              fn=1 * (!call.each.conc),
              fp=0)
        }
      }))
    }))
  }))
}

# apply the model loop to the fake correlated data
allfirst.poolct <- rbindlist(lapply(1:length(above.lod), experiment_loop, ct_dat = ct_fake_input_corr[,1]))

group_by(allfirst.poolct, pool, prevalence, above.llod, limit) %>% 
  summarize(pos=weighted.mean(pos, w=probability),
            total.tests=weighted.mean(tests, w=probability), 
            tests.per.sample=weighted.mean(tests, w=probability)/mean(pool), # calculate tests per sample
            tn=weighted.mean(tn, w=probability), 
            tp=weighted.mean(tp, w=probability), 
            fn=weighted.mean(fn, w=probability), 
            fp=weighted.mean(fp, w=probability)) -> apw

rm(allfirst.poolct)

all <- as.data.frame(apw)
rm(apw)

all2 <- all %>%
  mutate(ppa=tp/(tp+fn))
cpi <- as_tibble(BinomCI(all2$ppa*n,n,conf.level=0.95,method="clopper-pearson"))
allci <- bind_cols(all2,cpi)

write.csv(allci,"model_output_corr.csv")
```


```{r}
# MODIFIED Loop for Correlated Data 

# Model loop, vary % above each LOD
# experiment_loop_corr takes entire ct_dat data frame as argument- should have columns Ct and Z
experiment_loop_corr <- function(above, ct_dat){ 
  ct_dat$Ct <- ct_dat$Ct + mat2[above,3] # shift the Ct values by specified percent
  ct_set <- subset(ct_dat, ct_dat$Ct<45) # subset to valid Ct values
  # ct_set <- pmax(5,subset(ct_dat[,1] + mat2[above,3], ct_dat[,1] + mat2[above,3]<45))
  
  threshold.ct_index <- sapply(sample(ct_set$Z, n/G, replace = TRUE), function(x) {which(ct_set$Z == x)}) 
  # sample the indexes by group (i.e. always select all 4 members if a given group is sampled)
  t <- as.data.frame(table(threshold.ct_index)) 
  # frequency that each group is sampled in threshold.ct_index
  threshold.ct <- data.frame(Ct = rep(ct_set$Ct[t[,1]], t[,2]), Z = rep(1:(n/G), each = G)) 
  # replicate Ct values by number of times they appear in sampled indexes (number of times that group was sampled)
  
  # threshold.ct_index <- sapply(sample(ct_fake_input_corr$Z, n/G, replace = TRUE), function(x) {which(ct_fake_input_corr$Z == x)}) 
  # why do we sample with replacement? we end up including every sample, some multiple times - why?
  # sample indexes by group
  # don't we need to be sampling from ct_set?
	# threshold.ct <- data.frame(Ct = ct_fake_input_corr$Ct[threshold.ct_index, ], Z = rep(1:n/G, each = G)) 

  rbindlist(lapply(1:pool.max, function (p) { # pool size
    rbindlist(lapply(c(0.001,0.01,0.03,0.05,0.1,0.15), function(prevalence) { # proportion positive tests 
      rbindlist(lapply(0:p, function(positives) { # number of positives
        
        if (positives == 0) {
          data.frame(limit=lod, pool=p, pos=positives, prevalence=prevalence, 
                     above.llod = above.lod[above], 
                     concentration=0, probability = dbinom(positives, p, prevalence), random=0, z.index=0,
                     call.each.conc=FALSE, tests=1, tn=1,tp=0,fn=0,fp=0)
        } 
        else {
          # positives <- 5
          dat_index <- sapply(sample(threshold.ct$Z, positives * (n/G), replace=T), function(x) {which(threshold.ct$Z == x)})
          # sample data by group
          t <- as.data.frame(table(dat_index))
          dat <- matrix(rep(threshold.ct$Ct[t[,1]], t[,2]), nrow = positives)
          # dat <- matrix(threshold.ct$Ct[dat_index], nrow = positives)
          # sample data uniformly at random - introduce correlation structure here
          
          each.conc = -log2(colSums(2^-dat)/p)+ifelse(dilution.vary.index==1,0,rnorm(mean=0,sd=1.1,n=ncol(dat))) 
          # sd of 1.1 reflects confidence interval for deviation from perfect log2 dilution in assays
          
          data.frame(
            limit=lod,
            pool=p,
            pos=positives,
            prevalence=prevalence,
            above.llod=above.lod[above],
            concentration=each.conc,
            probability = dbinom(positives, p, prevalence)/n,
            random=sample(n,n,replace = TRUE)/n, #
            z.index=ifelse(probit.mode.index<4,probit.z.indices[probit.mode.index],
                           sample(1:length(z_scores),n,replace=T))) %>%
            mutate(
              call.each.conc=probit[1+(z.index-1)*571+each.conc*10-(lod-35.9)*10,2]>random,
              tests=1 + p * (call.each.conc), # number of tests done (number positive pools + pool size)
              tn=0,
              tp=1 * (call.each.conc),
              fn=1 * (!call.each.conc),
              fp=0)
        }
      }))
    }))
  }))
}

start_time <- Sys.time()
allfirst.poolct <- rbindlist(lapply(1:length(above.lod), experiment_loop_corr, ct_dat = ct_fake_input_corr))
end_time <- Sys.time()
end_time-start_time

group_by(allfirst.poolct, pool, prevalence, above.llod, limit) %>% 
  summarize(pos=weighted.mean(pos, w=probability),
            total.tests=weighted.mean(tests, w=probability), 
            tests.per.sample=weighted.mean(tests, w=probability)/mean(pool), # calculate tests per sample
            tn=weighted.mean(tn, w=probability), 
            tp=weighted.mean(tp, w=probability), 
            fn=weighted.mean(fn, w=probability), 
            fp=weighted.mean(fp, w=probability)) -> apw

rm(allfirst.poolct)

all <- as.data.frame(apw)
rm(apw)

all2 <- all %>%
  mutate(ppa=tp/(tp+fn))
cpi <- as_tibble(BinomCI(all2$ppa*n,n,conf.level=0.95,method="clopper-pearson"))
allci <- bind_cols(all2,cpi)

write.csv(allci,"model_output_corr_grouped.csv")
```



```{r}
## THIS STILL NEEDS TO BE MODIFIED
# Correlated Data
# Model loop, vary % above each LOD
# ALSO WILL NEED TO CHANGE LOOP for correlated data 
for (llod in 1:length(lod)){ # can we get rid of this outer loop? 

allfirst.poolct <- data.table::rbindlist(lapply(1:length(above.lod), # for each row in mat2 (1:6)
                                                function(above){
		ct_set<-pmax(5,subset(ct_fake_input_corr_t+mat2[(llod-1)*length(above.lod)+above,3],
		                      ct_fake_input_corr_t+mat2[(llod-1)*length(above.lod)+above,3]<45))
		#threshold.ct <- c(ct_fake_input_corr %>% group_by(Z) %>% sample_n(2)) # sample by groups; WITH REPLACEMENT?
	  threshold.ct_index <- sapply(sample(ct_fake_input_corr$Z, n/G, replace = TRUE), function(x) {which(ct_fake_input_corr$Z == x)}) # sample indexes
	  threshold.ct <- data.frame(Ct = ct_fake_input_corr$Ct[threshold.ct_index, ], Z = rep(1:n/G, each = G))
	   
		data.table::rbindlist(lapply(1:pool.max, function (p) { # pool size
		  data.table::rbindlist(lapply(c(0.001,0.01,0.03,0.05,0.1,0.15), function(prevalence) { # proportion positive tests 
		    data.table::rbindlist(lapply(0:p, function(positives) {

		      if (positives == 0) data.frame(limit=lod[llod], pool=p, pos=positives, prevalence=prevalence, above.llod = above.lod[above], 
		              concentration=0, probability = dbinom(positives, p, prevalence), random=0, z.index=0, call.each.conc=FALSE, tests=1, tn=1,tp=0,fn=0,fp=0)
		      
		      else {
				dat_index <- sapply(sample(threshold.ct$Z, positives * (n/G), replace=T), function(x) {which(threshold.ct$Z == x)})
				# might need to unlist after the sapply
			  dat <- matrix(threshold.ct$Ct[dat_index], nrow = positive)
				# sample data uniformly at random - introduce correlation structure here
		      
				each.conc = -log2(colSums(2^-dat)/p)+ifelse(dilution.vary.index==1,0,rnorm(mean=0,sd=1.1,n=ncol(dat))) # sd of 1.1 reflects confidence interval for deviation from perfect log2 dilution in assays
			
				data.frame(
					limit=lod[llod],
					pool=p,
					pos=positives,
					prevalence=prevalence,
					above.llod=above.lod[above],
					concentration=each.conc,
					probability = dbinom(positives, p, prevalence)/n,
					random=sample(n,n,replace = TRUE)/n, #
					z.index=ifelse(probit.mode.index<4,probit.z.indices[probit.mode.index],
					               sample(1:length(z_scores),n,replace=T))) %>%
					mutate(
						call.each.conc=probit[1+(z.index-1)*571+each.conc*10-(lod[llod]-35.9)*10,2]>random,
						tests=1 + p * (call.each.conc),
						tn=0,
						tp=1 * (call.each.conc),
						fn=1 * (!call.each.conc),
						fp=0)
        }
      }))
    }))
  }))
}))

group_by(allfirst.poolct, pool, prevalence, above.llod, limit) %>% 
  summarize(pos=weighted.mean(pos, w=probability),
            total.tests=weighted.mean(tests, w=probability), 
            tests.per.sample=weighted.mean(tests, w=probability)/mean(pool),
            tn=weighted.mean(tn, w=probability), 
            tp=weighted.mean(tp, w=probability), 
            fn=weighted.mean(fn, w=probability), 
            fp=weighted.mean(fp, w=probability)) -> apw

rm(allfirst.poolct)

tmp <- as.data.frame(apw)
ifelse(llod==1,all<-apw,all<-rbind(all,tmp))
rm(tmp)
rm(apw)
}

```


## Plots 



```{r generate_plots, eval=FALSE}
# Generate individual plots

# Print plot of pool size vs. PPA, panel grid of % samples with Ct > LoD, color by proportion test pos
pdf("plot.ppa.above.colprev.pdf",width=8, height=5)
ggplot(allci, aes(x=pool, y=tp/(tp + fn), color=as.factor(prevalence))) + 
  geom_line() +
  facet_wrap(vars(factor(above.llod, labels=c("5% samples Ct>LoD","10% samples Ct>LoD","15% samples Ct>LoD","20% samples Ct>LoD","25% samples Ct>LoD","30% samples Ct>LoD")))) +
  theme_bw() + 
  scale_color_brewer(palette="RdYlBu", name="Prevalence") + 
  xlab("Pool size") + ylab("Expected positive percent agreement") +
  scale_x_continuous(limits=c(1,20), breaks=seq(0,20,2)) + scale_y_continuous(limits=c(0.5,1),breaks=seq(0.5,1,.1)) +
  guides(color=guide_legend(title="Proportion of\ntests positive"))
dev.off()

# Print plot of pool size vs. PPA, panel grid of proportion test pos, color by % samples with Ct > LoD
pdf("plot.ppa.prev.colabove.pdf",width=8, height=5)
ggplot(allci, aes(x=pool, y=tp/(tp + fn), color=as.factor(above.llod))) + 
  geom_line() +
  facet_wrap(vars(factor(prevalence, labels=c("0.1% Tests positive", "1% Tests positive", "3% Tests positive", "5% Tests positive", "10% Tests positive","15% Tests positive")))) +
  theme_bw() + 
  scale_color_brewer(palette="RdYlBu", name="Prevalence") + 
  xlab("Pool size") + ylab("Expected positive percent agreement") +
  scale_x_continuous(limits=c(1,20), breaks=seq(0,20,2)) + scale_y_continuous(limits=c(0.5,1),breaks=seq(0.5,1,.1)) +
  guides(color=guide_legend(title="Proportion\nsamples\nCt > LoD"))
dev.off()

# Print plot of pool size vs. average tests/sample, panel grid of %Ct > LoD, color by proportion test pos
pdf("plot.eff.above.colprev.pdf",width=8, height=5)
ggplot(allci, aes(x=pool, y=tests.per.sample,color=as.factor(prevalence))) + 
  geom_line() +
  facet_wrap(vars(factor(above.llod, labels=c("5% samples Ct>LoD","10% samples Ct>LoD","15% samples Ct>LoD","20% samples Ct>LoD","25% samples Ct>LoD","30% samples Ct>LoD")))) +
  theme_bw() + 
  scale_color_brewer(palette="RdYlBu", name="Prevalence") + 
  xlab("Pool size") + ylab("Average tests per sample") + 
  scale_x_continuous(limits=c(1,20), breaks=seq(0,20,2)) + scale_y_continuous(trans="reverse", limits=c(1.2,0),breaks=seq(0,1.2,0.2)) +
  guides(color=guide_legend(title="Proportion of\ntests positive"))
dev.off()

# Print plot of pool size vs. PPA, gridded showing PPA changes with %>LoD but is constant at different LoDs
# grid <- read.csv("model_output_varylod.varydist.csv")
# grid <- as_tibble(grid)
# pdf("AppendixFigure2.pdf",width=9, height=8)
# ggplot(grid %>% filter((above.llod=="0.05"|above.llod=="0.15"|above.llod=="0.25"),(limit==34|limit==35|limit==36)), aes(x=pool, y=tp/(tp + fn), color=as.factor(prevalence))) + 
#   geom_line() + 
#   facet_grid(rows=vars(factor(limit,levels=c(36,35,34),labels=c("LoD Ct=36","LoD Ct=35","LoD Ct=34"))),cols=vars(factor(above.llod,labels=c("5% Ct>LoD","15% Ct>LoD","25% Ct>LoD")))) + 
#   theme_bw() + 
#   scale_color_brewer(palette="RdYlBu", name="Prevalence") + 
#   xlab("Pool size") + ylab("Expected positive percent agreement") + 
#   scale_x_continuous(limits=c(1,20),breaks=seq(0,20,2)) + scale_y_continuous(limits=c(0.5,1),breaks=seq(0.5,1,.1)) + 
#   guides(color=guide_legend(title="Proportion of\ntests positive"))
# dev.off()

```

```{r}
allci_corr <- read.csv("model_output_corr.csv")
allci_uncorr <- read.csv("model_output.csv")
```

```{r}
# Generate Plots with multiple data sets (correlated vs. uncorrelated)
# Print plot of pool size vs. PPA, panel grid of % samples with Ct > LoD, color by proportion test pos
pdf("plot.ppa.above.colprev.both.pdf",width=8, height=5)
ggplot() +
  geom_line(allci_corr, mapping = aes(x=pool, y=tp/(tp + fn), color=as.factor(prevalence))) +
  geom_line(allci_uncorr, mapping = aes(x=pool, y=tp/(tp + fn), color=as.factor(prevalence)), linetype = "dashed") +
  facet_wrap(vars(factor(allci_corr$above.llod, labels=c("5% samples Ct>LoD","10% samples Ct>LoD","15% samples Ct>LoD","20% samples Ct>LoD","25% samples Ct>LoD","30% samples Ct>LoD")))) +
  theme_bw() + 
  scale_color_brewer(palette="RdYlBu", name="Prevalence") + 
  xlab("Pool size") + ylab("Expected positive percent agreement") +
  scale_x_continuous(limits=c(1,20), breaks=seq(0,20,2)) + scale_y_continuous(limits=c(0.65,1),breaks=seq(0.65,1,.1)) +
  guides(color=guide_legend(title="Proportion of\ntests positive"))
dev.off()

# Print plot of pool size vs. PPA, panel grid of proportion test pos, color by % samples with Ct > LoD
pdf("plot.ppa.prev.colabove.both.pdf",width=8, height=5)
ggplot() + 
  geom_line(allci_corr, mapping = aes(x=pool, y=tp/(tp + fn), color=as.factor(above.llod))) +
  geom_line(allci_uncorr, mapping = aes(x=pool, y=tp/(tp + fn), color=as.factor(above.llod)), linetype = "dashed") +
  facet_wrap(vars(factor(allci_corr$prevalence, labels=c("0.1% Tests positive", "1% Tests positive", "3% Tests positive", "5% Tests positive", "10% Tests positive","15% Tests positive")))) +
  theme_bw() + 
  scale_color_brewer(palette="RdYlBu", name="Prevalence") + 
  xlab("Pool size") + ylab("Expected positive percent agreement") +
  scale_x_continuous(limits=c(1,20), breaks=seq(0,20,2)) + scale_y_continuous(limits=c(0.5,1),breaks=seq(0.5,1,.1)) +
  guides(color=guide_legend(title="Proportion\nsamples\nCt > LoD"))
dev.off()

# Print plot of pool size vs. average tests/sample, panel grid of %Ct > LoD, color by proportion test pos
pdf("plot.eff.above.colprev.both.pdf",width=8, height=5)
ggplot() + 
  geom_line(allci_corr, mapping = aes(x=pool, y=tests.per.sample,color=as.factor(prevalence))) +
  geom_line(allci_uncorr, mapping = aes(x=pool, y=tests.per.sample,color=as.factor(prevalence)), linetype = "dashed") +
  facet_wrap(vars(factor(allci_corr$above.llod, labels=c("5% samples Ct>LoD","10% samples Ct>LoD","15% samples Ct>LoD","20% samples Ct>LoD","25% samples Ct>LoD","30% samples Ct>LoD")))) +
  theme_bw() + 
  scale_color_brewer(palette="RdYlBu", name="Prevalence") + 
  xlab("Pool size") + ylab("Average tests per sample") + 
  scale_x_continuous(limits=c(1,20), breaks=seq(0,20,2)) + scale_y_continuous(trans="reverse", limits=c(1.2,0),breaks=seq(0,1.2,0.2)) +
  guides(color=guide_legend(title="Proportion of\ntests positive"))
dev.off()
```











---
title: "Working with Real Tau Data"
output: pdf_document
---

```{r message=FALSE, warning=FALSE}
#library(httr)
#library(jsonlite)
library(zoo)
library(dplyr)
library(ggplot2)
library(fitdistrplus)
```

# Tau 

Secondary attack (infection) rate: https://www.cdc.gov/csels/dsepd/ss1978/lesson3/section2.html

```{r}
path.in <- "5_SAR datasets" # Specify folder locatin of inputs
Lewis2020 <- read.csv(file.path(path.in, "Lewis2020.csv"))
Jing2020 <- read.csv(file.path(path.in, "Jing2020.csv"))
```

## Lewis et al 2020: 
- 58 households
- 32 households had evidence of secondary infection 
- SIR was 28\% (n=52/188) 
- Study of household transmission. Study was conducted March - May 2020 (period in which shelter-in-place orders and social distancing recommendations were largely in place across the US) in Utah and Wisconsin. These states chosen due to relatively low COVID-19 prevalence in order to reduce the risk of additional community exposure to household contacts. 

```{r}
# calculate SIR for entire study population 
sum(Lewis2020$any_pos)

table(Lewis2020$member_type) # member_type =1 is index case; = 0 is household contact

new_cases <- sum(Lewis2020$any_pos)- sum(Lewis2020$member_type)
new_cases / (nrow(Lewis2020) - sum(Lewis2020$member_type)) 
# 58 primary infections
# 52 secondary infections (numerator)
# 188 contacts (denominator)
# SIR = 28% (same as found in Lewis 2020 paper)
# SAR = number new cases among contacts / total number of contacts
```

Note: 
`new_hh_memnum` gives the total number of people in the household
However, 9 individuals declined to participate
`house_mem_count` gives the number of people in the household who participated
We will estimate SAR assuming the total size of the household is given by house_mem_count
```{r}
# create unique householdid var
Lewis2020$household_id <- gsub("(\\d+)[^-]*$", "", Lewis2020$study_id_merge)
Lewis2020$household_id <- gsub("-", "", Lewis2020$household_id)
# https://stackoverflow.com/questions/23641879/regular-expression-match-all-numbers-after-the-last-dash

```


Estimate a secondary infection rate for each household/cluster in the data set and then fit a distribution to these taus
```{r}
Lewis_SAR <- Lewis2020 %>% 
  group_by(household_id) %>%
  summarise(household_id = household_id,
            secondary = sum(any_pos)-1,
            num_contacts = housemem_count-1,
            SAR = secondary/ num_contacts, 
            data_id = "Lewis2020") %>%
            distinct()

```

## Jing et al 2020
```{r}
# for now, lets just look at household secondary attack rate
Jing2020_house <- Jing2020 %>% filter(Same.Residential.Address == 1)
Jing2020_house$any_pos <- ifelse(Jing2020_house$Case.Type == "primary case" | Jing2020_house$Case.Type == "secondary case", 
                                 1, 0)
# 14 times where there is more than one primary case in a cluster
Jing2020_house %>% count(Cluster.ID, Case.Type) %>% summarise(bad = sum(ifelse(Case.Type == "primary case" & n > 1, 1, 0)))
# cluster.id where there is more than one primary case
remove_clusters <- Jing2020_house %>% count(Cluster.ID, Case.Type) %>% filter(Case.Type == "primary case" & n > 1) %>% dplyr::select(Cluster.ID)

Jing2020_house2 <- subset(Jing2020_house, !Jing2020_house$Cluster.ID %in% remove_clusters$Cluster.ID)
Jing2020_house2$IsSecondary <- ifelse(Jing2020_house2$Case.Type == "secondary case", 1, 0)

temp <- Jing2020_house2 %>% count(Cluster.ID)
table(temp$n) # 19 clusters that only contain 1 person (primary case)
# these 19 clusters are dropped (can't infer valid SAR)
length(unique(Jing2020_house2$Cluster.ID)) # 168 clusters total

Jing_SAR <- Jing2020_house2 %>% 
  filter(Case.Type == "secondary case" | Case.Type == "contact") %>%
  group_by(Cluster.ID) %>%
  summarise(household_id = Cluster.ID,
            secondary = sum(IsSecondary), 
            num_contacts = Size..Same.Residential.Address.-1, 
            SAR = secondary/num_contacts, 
            data_id = "Jing2020") %>%
  distinct() # 149 clusters after dropping the 19 that only have 1 person

# SAR = number new cases among contacts / total number of contacts
```
#? 
in the paper says that SAR was calculated as the number of secondary cases divided by the sum of secondary cases and non-cases

## Fit distribution to data 
Merge into one SAR dataset
```{r}
# keep columns that are same in both data sets
common_col <- c(intersect(names(Jing_SAR), names(Lewis_SAR)))
all_SAR <- rbind(Jing_SAR[,common_col], Lewis_SAR[,common_col])
```

### EM Algorithm
1. Start with uniform prior on $\tau$: Beta(alpha=1, beta=1)

2. E step: impute $tau_g$ *for each cluster* using the mean of the beta posterior: 
$$
\tau_g = \frac{\alpha + s_g}{\alpha + \beta + N_g}
$$
where $s_g$ is the number of secondary cases and $N_g$ is the number of contacts in group $g$. 

3. M step: impute $\alpha$ and $\beta$ using the method of moment estimates for a Beta distribution

4. Repeat the E and M steps, updating $\alpha$ and $\beta$ each time until the parameter values converge, i.e. stabilize such that
$$
(\alpha_{old} - \alpha_{new})^2 + (\beta_{old} - \beta_{new})^2 < \epsilon
$$
```{r}
b <- 10
# initialize parameters
alpha <- numeric(b)
beta <- numeric(b)
xbar <- numeric(b)
vbar <- numeric(b)
alpha[1] <- 2
beta[1] <- 2

getmode <- function(v) {
   uniqv <- unique(v)
   uniqv[which.max(tabulate(match(v, uniqv)))]
}

for (i in 2:b){
  # E step
  tau_g <- (alpha[i-1] + all_SAR$secondary)/(alpha[i-1] + beta[i-1] + all_SAR$num_contacts)
  print(paste("tau_g: ", round(head(tau_g), 2)))
  
  # M step
  xbar[i] <- getmode(tau_g)
  #xbar[i] <- (1/length(tau_g))*sum(tau_g)
  vbar[i] <- (1/(length(tau_g)-1))*sum((tau_g - xbar[i])^2)
  print(paste("vbar= ", round(vbar[i], 3), "xbar*(1-xbar)= ", round(xbar[i]*(1-xbar[i]), 3)))
  
  alpha[i] <- xbar[i]*(((xbar[i]*(1-xbar[i]))/vbar[i])-1)
  beta[i] <- (1-xbar[i])*(((xbar[i]*(1-xbar[i]))/vbar[i])-1)
  print(paste("alpha= ", alpha[i], "beta= ", beta[i]))
  print((alpha[i-1]-alpha[i])^2 + (beta[i-1]-beta[i])^2)
}
```





The beta distribution is the conjugate prior probability distribution to the binomial and is a useful model for the random behavior of a percentage/proportion (since it is defined n the interval [0,1]). Given our observed SAR values (data, $X$) and a beta prior, we want to estimate the parameters (alpha and beta) 
may need: Bayesian updates (observing something sampled from binomial dist; want hyper parameters)
each tau is a RV sampled from beta, want to estimate alpha and beta
write down update from likelihood formula
```{r}
# Fit distribution
# Plot distribution and skew/kurtosis of standard deviation of the case density
plotdist(Lewis_SAR$SAR, histo = TRUE, demp = TRUE, breaks=18)
descdist(Lewis_SAR$SAR, boot = 1000)

# Fit distributions
fw <- fitdist(Lewis_SAR$SAR, "weibull")
fno <- fitdist(Lewis_SAR$SAR, "norm")
fg <- fitdist(Lewis_SAR$SAR, "gamma")
fln <- fitdist(Lewis_SAR$SAR, "lnorm")

# Plot fit distributions and generate goodness-of-fit statistics
par(mfrow = c(2, 2))
plot.legend <- c("weibull","normal","gamma", "lnorm")
denscomp(list(fw,fno,fg,fln), legendtext = plot.legend)
qqcomp(list(fw,fno,fg,fln), legendtext = plot.legend)
cdfcomp(list(fw,fno,fg,fln), legendtext = plot.legend)
ppcomp(list(fw,fno,fg,fln), legendtext = plot.legend)
gofstat(list(fw,fno,fg,fln))

## Beta distribution
# rescale data in interval [0,1] to fit beta distribution
# density_sd_dat_scaled <- density_sd_dat/10 
# # scaling by max(density_sd_dat) leads to fitdit() failing; 10 chosen somewhat aribitrarily
# fit all the distributions on the scaled data so that the results are comparable
# fb <- fitdistrplus::fitdist(density_sd_dat_scaled, "beta")
# fw <- fitdist(density_sd_dat_scaled, "weibull")
# fno <- fitdist(density_sd_dat_scaled, "norm")
# fg <- fitdist(density_sd_dat_scaled, "gamma")
# fln <- fitdist(density_sd_dat_scaled, "lnorm")

# par(mfrow = c(2, 2))
# plot.legend <- c("weibull","normal","gamma", "lnorm", "beta)
# denscomp(list(fw,fno,fg,fln, fb), legendtext = plot.legend,xlim=c(0,50))
# qqcomp(list(fw,fno,fg,fln, fb), legendtext = plot.legend)
# cdfcomp(list(fw,fno,fg,fln, fb), legendtext = plot.legend)
# ppcomp(list(fw,fno,fg,fln, fb), legendtext = plot.legend)
# gofstat(list(fw,fno,fg,fln, fb))

## beta ends up being very similar to the other distributions and does not perform any better
## so lets go back to the unscaled data  (as above)
```


# Prevalence 

Get data from COVID Act Now API: https://apidocs.covidactnow.org/api
COVID Act Now, API key: f21877deb3c9401399d1584884d175a8
Data definitions available here: https://apidocs.covidactnow.org/data-definitions

## All state timeseries and US time series data
```{r}
#dat <- read.csv("covidactnow.states.timeseries.csv")
# states_timeseries <- read.csv("https://api.covidactnow.org/v2/states.timeseries.csv?apiKey=f21877deb3c9401399d1584884d175a8")

us_timeseries <- read.csv("https://api.covidactnow.org/v2/country/US.timeseries.csv?apiKey=f21877deb3c9401399d1584884d175a8")
```

Download alternative data: https://covidtracking.com/data
```{r}
allstates_rollingavg <- read.csv("all-states-history_covidtrackingproject.csv")
```


### US Time Series Data
ACTIVE CASES
```{r}
read.csv(file=“https://raw.githubusercontent.com/owid/covid-19-data/master/public/data/owid-covid-data.csv”, header=T, sep=“,”)
COUNTRY_DATA$date <- (as.Date(COUNTRY_DATA$date, “%Y-%m-%d”))
```


```{r}
# remove unnecessary columns
us_ts <- us_timeseries %>% dplyr::select(date, contains("cases"), contains("case"))

# remove dates where actuals.newCases is missing (NA)
us_ts <- us_ts[!is.na(us_ts$actuals.newCases),]
```


- Compute seven-day rolling average of cases for the US overall
- `metrics.caseDensity` gives the number of cases per 100k population calculated using a 7-day rolling average. Compute the standard deviation of this rolling average.
- Compute standard deviation of seven day rolling avg. and `metrics.caseDensity`
```{r}
us_ts$date <- as.Date(us_ts$date)
us_ts <- us_ts %>%
   # dplyr::arrange(desc(state)) %>% 
    #dplyr::group_by(state) %>% 
    dplyr::mutate(cases_03da = zoo::rollmean(actuals.newCases, k = 3, fill = NA),
                  cases_05da = zoo::rollmean(actuals.newCases, k = 5, fill = NA),
                  cases_07da = zoo::rollmean(actuals.newCases, k = 7, fill = NA),
                  cases_15da = zoo::rollmean(actuals.newCases, k = 15, fill = NA),
                  cases_21da = zoo::rollmean(actuals.newCases, k = 21, fill = NA), 
                  cases_07da_sd = zoo::rollapply(cases_07da, width=7, 
                                                 FUN=function(x) sd(x, na.rm=FALSE), 
                                                 by =1, fill=NA), 
                  metrics.caseDensity_sd = zoo::rollapply(metrics.caseDensity,
                                                          width=7, 
                                                          FUN=function(x) sd(x, na.rm=FALSE),
                                                          by=1, fill=NA), 
                 year = factor(as.numeric(format(date, format = "%Y"))),
                 month = factor(as.numeric(format(date, format = "%m"))),
                 day = as.numeric(format(date, format = "%d"))) 
#%>% dplyr::ungroup()

#t <- us_timeseries[is.na(us_timeseries$cases_07da),]
#t2 <- us_timeseries[is.na(us_timeseries$actuals.newCases),]
```

Plot standard deviation as a function of prevalence
```{r}
ggplot(us_ts, aes(x=cases_07da, y=cases_07da_sd))+
  geom_line()

ggplot(us_ts, aes(x=metrics.caseDensity, y=metrics.caseDensity_sd))+
  geom_point(aes(color=month, shape = year)) +
  xlab("Cases per 100,000 (7-Day Rolling Avg.)")+
  ylab("Std. Dev. of Cases per 100,000 7-Day Rolling Avg.")

ggplot(us_ts, aes(x=metrics.caseDensity, y=metrics.caseDensity_sd))+
  geom_point(aes(color=month)) +
  facet_wrap(~year)+
  xlab("Cases per 100,000 (7-Day Rolling Avg.)")+
  ylab("Std. Dev. of Cases per 100,000 7-Day Rolling Avg.")
```

We observe that the higher the prevalence, the higher the sd. So variance is heteroscedastic. We want a model that reflects this. 

Better to use number of ACTIVE cases rather than number of new cases

model coefficient of variation (sd / mean): linear? fit distribution that matches these properties
don't take entire time series; generate coefficient of variation per week 
look at distribution of coefficient of variation as function of prevalence
If coefficient of variation is roughly linear, find distribution that roughly fits this criteria

Fit distributions to the real data 
```{r}
# remove missing metrics.caseDensity_sd values
# (missing for first and last 3 days of data set)
density_sd_dat <- us_ts[,"metrics.caseDensity_sd"]
density_sd_dat <- density_sd_dat[!is.na(density_sd_dat)]


# Fit distribution
# Plot distribution and skew/kurtosis of standard deviation of the case density
plotdist(density_sd_dat, histo = TRUE, demp = TRUE, breaks=18)
descdist(density_sd_dat, boot = 1000)

# Fit distributions
fw <- fitdist(density_sd_dat, "weibull")
fno <- fitdist(density_sd_dat, "norm")
fg <- fitdist(density_sd_dat, "gamma")
fln <- fitdist(density_sd_dat, "lnorm")

# Plot fit distributions and generate goodness-of-fit statistics
par(mfrow = c(2, 2))
plot.legend <- c("weibull","normal","gamma", "lnorm")
denscomp(list(fw,fno,fg,fln), legendtext = plot.legend)
qqcomp(list(fw,fno,fg,fln), legendtext = plot.legend)
cdfcomp(list(fw,fno,fg,fln), legendtext = plot.legend)
ppcomp(list(fw,fno,fg,fln), legendtext = plot.legend)
gofstat(list(fw,fno,fg,fln))

## Beta distribution
# rescale data in interval [0,1] to fit beta distribution
# density_sd_dat_scaled <- density_sd_dat/10 
# # scaling by max(density_sd_dat) leads to fitdit() failing; 10 chosen somewhat aribitrarily
# fit all the distributions on the scaled data so that the results are comparable
# fb <- fitdistrplus::fitdist(density_sd_dat_scaled, "beta")
# fw <- fitdist(density_sd_dat_scaled, "weibull")
# fno <- fitdist(density_sd_dat_scaled, "norm")
# fg <- fitdist(density_sd_dat_scaled, "gamma")
# fln <- fitdist(density_sd_dat_scaled, "lnorm")

# par(mfrow = c(2, 2))
# plot.legend <- c("weibull","normal","gamma", "lnorm", "beta)
# denscomp(list(fw,fno,fg,fln, fb), legendtext = plot.legend,xlim=c(0,50))
# qqcomp(list(fw,fno,fg,fln, fb), legendtext = plot.legend)
# cdfcomp(list(fw,fno,fg,fln, fb), legendtext = plot.legend)
# ppcomp(list(fw,fno,fg,fln, fb), legendtext = plot.legend)
# gofstat(list(fw,fno,fg,fln, fb))

## beta ends up being very similar to the other distributions and does not perform any better
## so lets go back to the unscaled data  (as above)
```
Log Normal (`lnorm`) has best fit based on the fit statistics. Extract fit parameters. 
```{r get_best_fit}
summary(fln)
meanlog= summary(fln)[[1]][1]
sdlog = summary(fln)[[1]][2]
```

CHECK: The log-normal distribution is often parameterised by the parameters for the corresponding normal distribution, hence why the estimate of the mean is negative. So we can generate data $Y \sim \mathcal{N}(\mu, \sigma^2)$, and then find the corresponding log-normal $X = exp(Y)$. 

https://stats.stackexchange.com/questions/126397/lognormal-with-negative-mean

Fitting parametric distributions in R: 
https://www.r-project.org/conferences/useR-2009/slides/Delignette-Muller+Pouillot+Denis.pdf





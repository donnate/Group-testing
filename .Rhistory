test_performed$Date
as.Date("10/21/20",format = "%m-%d-%Y")
as.Date("10/21/20",format = "%M-%d-%Y")
as.Date("10/21/20",format = "%m-%d-%y")
as.Date("10/21/20",format = "%m/%d/%y")
test_performed$Date = as.Date(test_performed$Date, format = "%m/%d/%y")
test_performed$Date
coldate = test_performed[which(test_performed$Entity == country),]
country
coldate
x0 = which(abs(coldate-date0) == min(abs(coldate - date0)))
coldate
coldate = test_performed[which(test_performed$Entity == country),"Date"]
x0 = which(abs(coldate-date0) == min(abs(coldate - date0)))
coldate
which(abs(coldate$Date-date0) == min(abs(coldate$Date - date0)))
coldate = test_performed[which(test_performed$Entity == country),]
x0 = which(abs(coldate$Date-date0) == min(abs(coldate$Date - date0)))
coldate[x0,]
which(!is.null(colgate[x0,"Code"]))
which(!is.null(coldate[x0,"Code"]))
database_lookup = df[which((df$country_name == country) & (df$region == region)),]
pop = database_lookup$pop
pop
(coldate$new_tests_per_thousand_7day_smoothed[x0] * 1000)/pop
if (length(x0)>1){
x0 = x0[which(!is.null(coldate[x0,"Code"]))]
}
(coldate$new_tests_per_thousand_7day_smoothed[x0] * 1000)/pop
coldate = fb_data[which(fb_data$country == country & fb_data$region == abbr),]
coldate = fb_data[which(fb_data$country == country & fb_data$region == ""),]
coldate
View(coldate)
date()
date
date0
coldate = test_performed[which(test_performed$Entity == country),]
coldate
coldate[x0,]
2.46 * 66.99
2.46 / 66.99
pop
2.46 / 669977
2.46 / 66977
date0-30
test_performed[which((test_performed$Date > date0-30) & (test_performed$Date < date0+30)),]
sd_pct_tested = test_performed[which((test_performed$Entity == country) & (test_performed$Date > date0-30) & (test_performed$Date < date0+30)),]
sd_pct_tested
sd(test_performed[which((test_performed$Entity == country) & (test_performed$Date > date0-30) & (test_performed$Date < date0+30)),"new_tests_per_thousand_7days"] *1000)
sd_pct_tested = sd(test_performed[which((test_performed$Entity == country) & (test_performed$Date > date0-30) & (test_performed$Date < date0+30)),"new_tests_per_thousand_7day_smoothed"] *1000)
test_performed[which((test_performed$Entity == country) & (test_performed$Date > date0-30) & (test_performed$Date < date0+30)),"new_tests_per_thousand_7day_smoothed"]
test_performed[which((test_performed$Entity == country) & (test_performed$Date > date0-30) & (test_performed$Date < date0+30)),"new_tests_per_thousand_7day_smoothed"] *1000)
test_performed[which((test_performed$Entity == country) & (test_performed$Date > date0-30) & (test_performed$Date < date0+30)),"new_tests_per_thousand_7day_smoothed"] *1000
test_performed[which((test_performed$Entity == country) & (test_performed$Date > date0-30) & (test_performed$Date < date0+30)),"new_tests_per_thousand_7day_smoothed"] * 1000/pop
sd(test_performed[which((test_performed$Entity == country) & (test_performed$Date > date0-30) & (test_performed$Date < date0+30)),"new_tests_per_thousand_7day_smoothed"] * 1000/pop)
sd(unlist(test_performed[which((test_performed$Entity == country) & (test_performed$Date > date0-30) & (test_performed$Date < date0+30)),"new_tests_per_thousand_7day_smoothed"] * 1000/pop))
fb_data$date
sd(fb_data[which(fb_data$country == country & fb_data$region == abbr),"pct_ili"])
abbr=""
sd(fb_data[which(fb_data$country == country & fb_data$region == abbr),"pct_ili"])
sd_ili = sd(unlist(fb_data[which(fb_data$country == country & fb_data$region == abbr),"pct_ili"]))
sd_ili
source('~/Dropbox/C3AI/preprocessing.R')
source('~/Dropbox/C3AI/preprocessing.R')
fetch_all_prevalence("Canada","British Columbia",  date0)
country = "Canada"; region ="British Columbia"
database_lookup = df[which((df$country_name == country) & (df$region == region)),]
pop = database_lookup$pop
database = database_lookup$database
name = database_lookup$country
abbr = ifelse (country == "United States", (states_abbreviations %>% filter(state == region))$state_abb,
ifelse(((region == "American Samoa") ||(country == "Puerto Rico,U.S.")), country, ""))
#### Fetch the FB data
month = paste0(format(date0,"%m"), '_', mymonths[as.numeric(format(date0,"%m"))])
fb_data = read_csv(paste0("FB_data/all_countries_", month,".csv"),
col_types = list(col_date(format = ""),
col_character(), col_double(),
col_double(),col_double(),col_character()))
fb_data$region[which(is.na(fb_data$region))] = ""
fb_data$region[which(fb_data$country =="Puerto Rico, U.S.")] = "Puerto Rico"
fb_data$region[which(fb_data$country =="American Samoa")] = "American Samoa"
fb_data$country[which(fb_data$country == "UnitedStates")] = "United States"
fb_data$country[which(fb_data$country =="Puerto Rico, U.S.")] = "United States"
fb_data$country[which(fb_data$country =="American Samoa")] = "United States"
fb_data$region[which(fb_data$country =="Hong Kong")] = "Hong Kong"
fb_data$country[which(fb_data$country == "Hong Kong")] = "China"
fb_data$country[which(fb_data$country == "Antigua")] = "Antigua and Barbuda"
fb_data$country[which(fb_data$country == "Palestine")] = "West Bank and Gaza"
if (country %in% unique(test_performed$Entity)){
coldate = test_performed[which(test_performed$Entity == country),]
x0 = which(abs(coldate$Date-date0) == min(abs(coldate$Date - date0)))
if (length(x0)>1){
x0 = x0[which(!is.null(coldate[x0,"Code"]))]
}
pct_tested = (coldate$new_tests_per_thousand_7day_smoothed[x0] * 1000)/pop
#### Capture the uncertainty
sd_pct_tested = sd(unlist(test_performed[which((test_performed$Entity == country) & (test_performed$Date > date0 -30)
& (test_performed$Date < date0+30)),
"new_tests_per_thousand_7day_smoothed"] * 1000/pop))
}else{
#### Try to get the test rate across the world
coldate = test_performed
x0 = which(abs(coldate$Date-date0) == min(abs(coldate$Date - date0)))
if (length(x0)>1){
x0 = x0[which(!is.null(coldate[x0,"Code"]))]
}
pct_tested = (mean(coldate$new_tests_per_thousand_7day_smoothed[x0]) * 1000)/pop
#### Capture the uncertainty
sd_pct_tested = sd(unlist(test_performed[which((test_performed$Date > date0-30)
& (test_performed$Date < date0+30)),
"new_tests_per_thousand_7day_smoothed"] * 1000/pop))
}
pct_tested
sd_pct_tested
if (country %in% unique(fb_data$country)){
coldate = fb_data[which(fb_data$country == country & fb_data$region == abbr),]
}else{
#### Try to get the test rate
coldate = fb_data
}
x0 = which(abs(coldate-date0) == min(abs(coldate - date0)))
coldatex0 = which(abs(coldate$date-date0) == min(abs(coldate$date - date0)))
x0 = which(abs(coldate$date-date0) == min(abs(coldate$date - date0)))
x0 = which(abs(coldate$date-date0) == min(abs(coldate$date - date0)))
sd_ili = sd(unlist(fb_data[which(fb_data$country == country & fb_data$region == abbr),"pct_ili"]))
sd_cli = sd(unlist(fb_data[which(fb_data$country == country & fb_data$region == abbr),"pct_cli"]))
fb_data = fb_data[x0,]
pct_ili = fb_data$pct_ili
pct_cli = fb_data$pct_cli
pct_ili
pct_cli
#### Fetch cases on that day
casecounts <- evalmetrics(
"outbreaklocation",
list(
spec = list(
ids = list(name),
expressions = list(paste0(database, "_ConfirmedCases")),
start = date0 - 31,
end = date0,
interval = "DAY"
)
)
)
casecounts = casecounts %>% mutate(new_cases = data - lag(data, default = 1)) %>% filter(dates >= date0 - 21)
casecounts = casecounts %>% mutate(smoothed_cases=rollapply(new_cases, 7, mean, align='right',fill=NA))  ### thats the new cases counts smoothed over
### Let's now compute the prevalence: active cases over the last 14 days
casecounts
prevalence = sum(casecounts %>% dplyr::filter(dates > date0 -14) %>% select(smoothed_cases))/ pop
prevalence
prevalence/pct_tested
prevalence/pct_cli
prevalence/(0.01* pct_cli)
pct_cli
prevalence
pct_tested
source('~/Dropbox/C3AI/preprocessing.R')
fetch_all_prevalence("Canada","British Columbia",  date0)
source('~/Dropbox/C3AI/preprocessing.R')
fetch_all_prevalence("Canada","British Columbia",  date0)
fetch_all_prevalence("Canada","Ontario",  date0)
source('~/Dropbox/C3AI/preprocessing.R')
fetch_all_prevalence("Canada","Ontario",  date0)
fb_data$pct_tested
source('~/Dropbox/C3AI/preprocessing.R')
fetch_all_prevalence("Canada","Ontario",  date0)
fetch_all_prevalence("Canada","Ontario",  date0)
fetch_all_prevalence("Saint Vincent and the Grenadines","Ontario",  date0)
fetch_all_prevalence("Saint Vincent and the Grenadines","Main territory",  date0)
country = "Saint Vincent and the Grenadines"
if (country %in% unique(fb_data$country)){
coldate = fb_data[which(fb_data$country == country & fb_data$region == abbr),]
}else{
#### Try to get the test rate
coldate = fb_data
}
country %in% unique(fb_data$country
)
sd(unlist(fb_data[which(fb_data$country == country & fb_data$region == abbr),"pct_ili"]))
(unlist(fb_data[which(fb_data$country == country & fb_data$region == abbr),"pct_ili"]))
source('~/Dropbox/C3AI/preprocessing.R')
fetch_all_prevalence("Saint Vincent and the Grenadines","Main territory",  date0)
source('~/Dropbox/C3AI/preprocessing.R')
source('~/Dropbox/C3AI/preprocessing.R')
fetch_all_prevalence("Saint Vincent and the Grenadines","Main territory",  date0)
runApp()
runApp()
runApp()
source('~/Dropbox/C3AI/preprocessing.R')
runApp()
source('~/Dropbox/C3AI/preprocessing.R')
source('~/Dropbox/C3AI/preprocessing.R')
fetch_all_prevalence("Sri Lanka", region,  as.Date("2020-05-01"))
fetch_all_prevalence("France", region,  as.Date("2020-05-01"))
fetch_all_prevalence("France", "Main territory",  as.Date("2020-05-01"))
fetch_all_prevalence("Sri Lanka", "Main territory",  as.Date("2020-05-01"))
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
rnorm(10000,0.01, 0.01)
hist(rnorm(10000,0.01, 0.01))
runApp()
runApp()
source('~/Dropbox/C3AI/preprocessing.R')
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
hist(rnorm(10000,0.01, 0.01))
runApp()
hist(rnorm(10000,0.01, 0.01))
runApp()
runApp()
hist(rnorm(10000,0.01, 0.01))
runApp()
hist(rnorm(10000,0.01, 0.01))
runApp()
hist(rnorm(10000,0.01, 0.01))
runApp()
hist(rnorm(10000,0.01, 0.01))
runApp()
hist(rnorm(10000,0.01, 0.01))
runApp()
runApp()
runApp()
runApp()
hist(rnorm(10000,0.01, 0.01))
runApp()
hist(rnorm(10000,0.01, 0.01))
source('~/Dropbox/C3AI/preprocessing.R')
### Let's now compute the prevalence: active cases over the last 14 days
prevalence = sum(casecounts %>% dplyr::filter(dates > date0 -14) %>% select(smoothed_cases))/ pop
runApp()
### Let's now compute the prevalence: active cases over the last 14 days
prevalence = sum(casecounts %>% dplyr::filter(dates > date0 -14) %>% select(smoothed_cases))/ pop
runApp()
runApp()
source('~/Dropbox/C3AI/preprocessing.R')
runApp()
### Let's now compute the prevalence: active cases over the last 14 days
prevalence = sum(casecounts %>% dplyr::filter(dates > date0 -14) %>% select(smoothed_cases))/ pop
runApp()
runApp('~/Dropbox/aerosol_transmission_model')
compute_hospitalization_probability <- function(age, Pregnant, Chronic_Renal_Insufficiency,
Diabetes, Immunosuppression, COPD, Obesity,
Hypertension, Tobacco, Cardiovascular_Disease,
Asthma, Gender){
a = age * 1.000
a = a + Pregnant * 0.172
a = a +  Chronic_Renal_Insufficiency * 	0.167
a = a + Diabetes * 0.165
a = a +  Immunosuppression *	0.139
a = a +  COPD	* 0.094
a = a + Obesity * 0.083
a = a +  Hypertension	 * 0.039
a = a + Tobacco * 0.007
a = a + Cardiovascular_Disease *	- 0.005
a = a + Asthma * 	- 0.065
a = a + Gender * -0.121
return(1/(1+exp(-a)))
}
compute_hospitalization_probability <- function(age, Pregnant, Chronic_Renal_Insufficiency,
Diabetes, Immunosuppression, COPD, Obesity,
Hypertension, Tobacco, Cardiovascular_Disease,
Asthma, Gender){
a = age * 1.000
a = a + Pregnant * 0.172
a = a +  Chronic_Renal_Insufficiency * 	0.167
a = a + Diabetes * 0.165
a = a +  Immunosuppression *	0.139
a = a +  COPD	* 0.094
a = a + Obesity * 0.083
a = a +  Hypertension	 * 0.039
a = a + Tobacco * 0.007
a = a + Cardiovascular_Disease *	- 0.005
a = a + Asthma * 	- 0.065
a = a + Gender * -0.121
return(1/(1+exp(-a)))
}
compute_hospitalization_probability(28, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1)
compute_hospitalization_probability(28, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1)
compute_hospitalization_probability(28, 1, 0, 0,10, 0, 0, 0, 0, 0, 0, 1)
compute_hospitalization_probability(1, 1, , 0,10, 0, 0, 0, 0, 0, 0, 1)
compute_hospitalization_probability(1, 1, 0, 0,10, 0, 0, 0, 0, 0, 0, 1)
compute_hospitalization_probability(1, 1, 0, 0,1, 0, 0, 0, 0, 0, 0, 1)
compute_hospitalization_probability(1, 1, 0, 0,0, 0, 0, 0, 0, 0, 0, 1)
compute_hospitalization_probability(1, 0, 0, 0,0, 0, 0, 0, 0, 0, 0, 1)
compute_hospitalization_probability(2, 0, 0, 0,0, 0, 0, 0, 0, 0, 0, 1)
compute_hospitalization_probability(2, 0, 0, 0,0, 0, 0, 0, 0, 0, 0, 0)
source('~/Dropbox/aerosol_transmission_model/individual_probabilities.R')
compute_hospitalization_probability(2, 0, 0, 0,0, 0, 0, 0, 0, 0, 0, 1)
compute_hospitalization_probability(2, 0, 0, 0,0, 0, 0, 0, 0, 0, 0, 1)
source('~/Dropbox/aerosol_transmission_model/individual_probabilities.R')
compute_hospitalization_probability(2, 0, 0, 0,0, 0, 0, 0, 0, 0, 0, 1)
compute_hospitalization_probability(2, 0, 0, 0,0, 0, 0, 0, 0)
compute_hospitalization_probability(2, 0, 0, 0,0, 0, 0, )
compute_hospitalization_probability(2, 0, 0, 0,0, 0, 0 )
compute_hospitalization_probability(27, 0, 0, 0,0, 0, 0 )
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
help(tags$a)
help(tags)
?tags$a
runApp()
runApp()
help("titlePanel")
runApp()
runApp()
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(ggplot2)
library(fitdistrplus)
library(DescTools)
library(data.table)
# Get all tests
tests <- data.table::fread("alltests_1mar24jun_v1.csv")
# Keep only valid positive ct values for first tests
tests %>% filter(result == "positive", firsttest==TRUE,!is.na(cttarget)) %>% pull(cttarget) -> cts
# Fit distribution
# Plot distribution and skew/kurtosis of ct values of real first tests
plotdist(cts, histo = TRUE, demp = TRUE, breaks=18)
descdist(cts, boot = 1000)
# Fit distributions
fw <- fitdist(cts, "weibull")
fno <- fitdist(cts, "norm")
fg <- fitdist(cts, "gamma")
fln <- fitdist(cts, "lnorm")
# Plot fit distributions and generate goodness-of-fit statistics
par(mfrow = c(2, 2))
plot.legend <- c("weibull","normal","gamma", "lnorm")
denscomp(list(fw,fno,fg,fln), legendtext = plot.legend,xlim=c(0,50))
qqcomp(list(fw,fno,fg,fln), legendtext = plot.legend)
cdfcomp(list(fw,fno,fg,fln), legendtext = plot.legend)
ppcomp(list(fw,fno,fg,fln), legendtext = plot.legend)
gofstat(list(fw,fno,fg,fln))
## GENERATE UN-CORRELATED DATA
# Add variable of proportion Ct value >LoD to model, as surrogate for differences in viral load distribution in different populations
# Number of replicates to generate ecdf offset values for above.
set.seed(42)
# x<-50000
x<-5000
# x <- 500
# Determine ct value quantiles for real first tests vs. fitted Weibull distribution
quantile(cts,c(0.5,0.25,0.75,0.95,0.99))
quantile(fw,c(0.5,0.25,0.75,0.95,0.99))
# Create simulated vector of ct values with same shape as that from Weibull distribution fit to ct values from real first tests
# 5000 draws from weibull with shape and scale parameters drawn from real data
ct_fake_input <- rweibull(x,fw[[1]][1],fw[[1]][2])
# Create matrix of desired input parameters
# Change above.lod to % samples with ct value >LoD to reflect actual population of interest. Changing "lod" itself has no effect on model output.
lod <- 35
above.lod <- seq(0.05,0.3,0.05)
translation_vector <- seq(-10,15,0.01)
mat<-matrix(ncol=3,nrow=length(translation_vector))
# Loop 1: fix LOD; for known shift, what percent of samples are above LOD  (a)?
for(v in 1:length(translation_vector)){
i=1
fn<-ecdf(subset(pmax(5,ct_fake_input+translation_vector[v]),
(pmax(5,ct_fake_input+translation_vector[v]))<45)) ### selects adequate values within the sample
a<-1-fn(lod) ###percentage of cvalues in the distribution above the LOD
mat[(v-1)+i,1]<-lod
mat[(v-1)+i,2]<-a
mat[(v-1)+i,3]<-translation_vector[v]
}
# Shift ct values for each lod and %above lod
mat2<-matrix(ncol=3,nrow=length(above.lod))
# Loop 2: fix LOD; for known a, how much should you shift?
for(j in 1:length(above.lod)){
tmp<-subset(mat,mat[,1]==lod)
u<-tmp[which.min(abs(above.lod[j]-tmp[,2])),3]
mat2[j,1]<-lod[i]
mat2[j,2]<-above.lod[j]
mat2[j,3]<-u
}
## GENERATE CORRELATED DATA
source("generating_correlated_data.R") # contains fxn create_correlated_Cts
# Create simulated vector of ct values with same shape as that from Weibull distribution fit to ct values from real first tests
###  Create fake correlated Cts
G <-  4 # group size
N <-  x
# creating a single group of correlated data from Weibull dist
ct_fake_input_corr <- data.frame("Ct" = create_correlated_Cts(type="block", N=G, rho=0.8),
"Z" = rep(1,G))
# creating 50000 data points, in correlated groups of size G
# 50,000 data points total, groups of size 4 (12,500 groups)
for (i in 2:(N/G)){ # N%/%G?
ct_fake_input_corr <- rbind(ct_fake_input_corr,
data.frame("Ct" =create_correlated_Cts(type="block", N=G, rho=0.8),
"Z" = rep(i,G))
)
}
# probit data input
probit_input <- read.csv("probit_zscores_cts_tissue_agnostic.csv")
probit_t<-subset(probit_input,probit_input$z_value<=1.96 & probit_input$z_value>=-1.96)
probit<-probit_t[,c(2,4,3)]
probit<-probit[order(probit$z_value,probit$ct_value),]
z_scores<-as.numeric(unlist(distinct(probit,z_value)))
# Number of replicates for model
set.seed(42)
n <- 10000
pool.max<-20
probit.mode<-c("base","dsa.lower","dsa.upper","psa")
probit.mode.index<-1 # 1 = no variation, 2, = LLN, 3 = ULN, 4 = probabilistic
probit.z.indices<-c(488,1,length(z_scores)) # 488 is a z score of 0 (base case) in the z index vector
dilution.vary.index<-1 # 1 = no variation, 2 = probabilistic
View(probit_input)
hist(probit_input$probit_probability_detection)
scatter(probit_input$ct_value, probit_input$probit_probability_detection)
plot(probit_input$ct_value, probit_input$probit_probability_detection)
source('~/Dropbox/Group-testing/dsitribution_assignments.R')
compute_assignments(2, 10, 3)
m=3; p=5; K= 3
N = p * K
if (m>N){
print("Error: m must be < than N")
return(NA)
}
if(m==0){
return(rep(0,K))
}
if(K ==1){
return(m)
}
res = c()
it = 1
for (i in 0:min(p,m)){
new_entries = compute_assignments(m-i,p, K-1)
for (e in new_entries){
res[[it]] <- c(i, e)
it <- it + 1
}
}
i
new_entries = compute_assignments(m-i,p, K-1)
new_entries = compute_assignments(m-i,p, 1)
new_entries
K=2
new_entries = compute_assignments(m-i,p, K-1)
new_entries
for (e in new_entries){print(e)}
c(i, e)
res[[it]] <- c(i, e)
res[[it]]
it = 1
for (i in 0:min(p,m)){
new_entries = compute_assignments(m-i,p, K-1)
for (e in new_entries){
res[[it]] <- c(i, e)
it <- it + 1
}
}
res
K=3
new_entries res[[1]]
new_entries = res[[1]]
new_entries
new_entries = res
for (e in new_entries){print(e)}
for (e in new_entries){
res[[it]] <- c(i, e)
it <- it + 1
}
res
K=3
source('~/Dropbox/Group-testing/dsitribution_assignments.R')
compute_assignments(2, 10, 3)
compute_assignments(2, 10, 3)
rep(0,2)
source('~/Dropbox/Group-testing/dsitribution_assignments.R')
compute_assignments(2, 10, 3)
source('~/Dropbox/Group-testing/dsitribution_assignments.R')
compute_assignments(2, 10, 3)
source('~/Dropbox/Group-testing/dsitribution_assignments.R')
compute_assignments(2, 10, 3)
K=3
i=2
res = c()
it = 1
new_entries = compute_assignments(m-i,p, K-1)
for (e in new_entries){print(e)}
i
m-i
m
m=2
new_entries = compute_assignments(m-i,p, K-1)
new_entries
source('~/Dropbox/Group-testing/dsitribution_assignments.R')
compute_assignments(2, 10, 3)
list(rep(0,K))
source('~/Dropbox/Group-testing/dsitribution_assignments.R')
source('~/Dropbox/Group-testing/dsitribution_assignments.R')
compute_assignments(2, 10, 3)
compute_assignments(10, 20, 5)
shiny::runApp('~/Dropbox/Group-testing')
runApp('~/Dropbox/Group-testing')

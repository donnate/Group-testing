if (positives == 0) {
# prevalence_corr <-  1-((1-prevalence)*(1-(prevalence*tau))^(p-1))
data.frame(limit=lod, pool=p, pos=positives, prevalence=prevalence,
above.llod = above.lod[above],
concentration=0,
tau = tau,
tau_relative_var = TAU_RELATIVE_VAR,
n_eff = unique(sim_probs$n_eff),
probability = as.numeric(sim_probs[which(sim_probs$n == 0), "p"]),
#probability_subset1 =  sim_subset_probs[which(sim_subset_probs$n == 0), "p"],
probability_null_test = dbinom(positives, p, prevalence), # uncorrected prevalence
prevalence_corr = sim_probs$pi_eff[1],
probability_null = sim_probs$prob_null_eff[1], # prevalence corrected for correlated individuals
random=0,
z.index=0,
call.each.conc=FALSE, tests=1, tn=1,tp=0,fn=0,fp=0)
}
else {
dat <- matrix(sample(threshold.ct, positives * n, replace=T), nrow=positives)
# sample data uniformly at random
# n samples of positives, rearrange into a matrix of positive rows, n columns
each.conc = -log2(colSums(2^-dat)/p)+ifelse(dilution.vary.index==1,0,
rnorm(mean=0,sd=1.1,n=ncol(dat)))
# sd of 1.1 reflects confidence interval for deviation from perfect log2 dilution in assays
# calculation dilution based on number of positives (colSum) in total pool size (p)
z.index= probit.z.indices[probit.mode.index]
t = data.frame(
limit=lod, pool=p, pos=positives, prevalence=prevalence,
above.llod = above.lod[above],
concentration=0,
tau = tau,
tau_relative_var = TAU_RELATIVE_VAR,
n_eff = sim_probs$n_eff[1],
probability = as.numeric(sim_probs[which(sim_probs$n == positives), "p"]),
#probability_subset1 =  sim_subset_probs[which(sim_subset_probs$n == 0), "p"],
probability_null_test = dbinom(positives, p, prevalence), # uncorrected prevalence
prevalence_corr = sim_probs$pi_eff,
probability_null = sim_probs$prob_null_eff, # prevalence corrected for correlated individuals
random=sample(n,n,replace = TRUE)/n, #
z.index=ifelse(probit.mode.index<4,probit.z.indices[probit.mode.index],
sample(1:length(z_scores),n,replace=T))) %>%
mutate(
call.each.conc=probit[1+(z.index-1)*571+each.conc*10-(lod-35.9)*10,2]>random,
tests=1 + p * (call.each.conc), # number of tests done (number positive pools + pool size)
tn=0,
tp=1 * (call.each.conc),
fn=1 * (!call.each.conc),
fp=0)
return(t)
}
}))
tt = data.frame(limit=lod, pool=p, pos=0, prevalence=prevalence,
above.llod = above.lod[above],
concentration=0,
tau = tau,
tau_relative_var = TAU_RELATIVE_VAR,
n_eff = unique(sim_probs$n_eff),
probability = as.numeric(sim_probs[which(sim_probs$n == 0), "p"]),
#probability_subset1 =  sim_subset_probs[which(sim_subset_probs$n == 0), "p"],
probability_null_test = dbinom(positives, p, prevalence), # uncorrected prevalence
prevalence_corr = sim_probs$pi_eff[1],
probability_null = sim_probs$prob_null_eff[1], # prevalence corrected for correlated individuals
random=0,
z.index=0,
call.each.conc=FALSE, tests=1, tn=1,tp=0,fn=0,fp=0)
positives =1
dat <- matrix(sample(threshold.ct, positives * n, replace=T), nrow=positives)
# sample data uniformly at random
# n samples of positives, rearrange into a matrix of positive rows, n columns
each.conc = -log2(colSums(2^-dat)/p)+ifelse(dilution.vary.index==1,0,
rnorm(mean=0,sd=1.1,n=ncol(dat)))
# sd of 1.1 reflects confidence interval for deviation from perfect log2 dilution in assays
# calculation dilution based on number of positives (colSum) in total pool size (p)
z.index= probit.z.indices[probit.mode.index]
t = data.frame(
limit=lod, pool=p, pos=positives, prevalence=prevalence,
above.llod = above.lod[above],
concentration=0,
tau = tau,
tau_relative_var = TAU_RELATIVE_VAR,
n_eff = sim_probs$n_eff[1],
probability = as.numeric(sim_probs[which(sim_probs$n == positives), "p"]),
#probability_subset1 =  sim_subset_probs[which(sim_subset_probs$n == 0), "p"],
probability_null_test = dbinom(positives, p, prevalence), # uncorrected prevalence
prevalence_corr = sim_probs$pi_eff,
probability_null = sim_probs$prob_null_eff, # prevalence corrected for correlated individuals
random=sample(n,n,replace = TRUE)/n, #
z.index=ifelse(probit.mode.index<4,probit.z.indices[probit.mode.index],
sample(1:length(z_scores),n,replace=T))) %>%
mutate(
call.each.conc=probit[1+(z.index-1)*571+each.conc*10-(lod-35.9)*10,2]>random,
tests=1 + p * (call.each.conc), # number of tests done (number positive pools + pool size)
tn=0,
tp=1 * (call.each.conc),
fn=1 * (!call.each.conc),
fp=0)
dat <- matrix(sample(threshold.ct, positives * n, replace=T), nrow=positives)
# sample data uniformly at random
# n samples of positives, rearrange into a matrix of positive rows, n columns
each.conc = -log2(colSums(2^-dat)/p)+ifelse(dilution.vary.index==1,0,
rnorm(mean=0,sd=1.1,n=ncol(dat)))
# sd of 1.1 reflects confidence interval for deviation from perfect log2 dilution in assays
# calculation dilution based on number of positives (colSum) in total pool size (p)
z.index= probit.z.indices[probit.mode.index]
t = data.frame(
limit=lod, pool=p, pos=positives, prevalence=prevalence,
above.llod = above.lod[above],
concentration=0,
tau = tau,
tau_relative_var = TAU_RELATIVE_VAR,
n_eff = sim_probs$n_eff[1],
probability = as.numeric(sim_probs[which(sim_probs$n == positives), "p"]),
#probability_subset1 =  sim_subset_probs[which(sim_subset_probs$n == 0), "p"],
probability_null_test = dbinom(positives, p, prevalence), # uncorrected prevalence
prevalence_corr = sim_probs$pi_eff,
probability_null = sim_probs$prob_null_eff, # prevalence corrected for correlated individuals
random=sample(n,n,replace = TRUE)/n, #
z.index=ifelse(probit.mode.index<4,probit.z.indices[probit.mode.index],
sample(1:length(z_scores),n,replace=T)))
n
sim_probs$n_eff[1]
above.lod[above]
as.numeric(sim_probs[which(sim_probs$n == positives), "p"])
sim_subset_probs[which(sim_subset_probs$n == 0), "p"]
dbinom(positives, p, prevalence)
sim_probs$prob_null_eff
sim_probs$pi_eff
sim_probs$prob_null_eff
sim_probs
sim_probs$p
sum(sim_probs$p)
sum(sim_probs$prob_null_eff)
1-sim_probs$prob_null_eff[1]
dbinom(positives, p, prevalence)
sim_probs$prob_null_eff[positives+1]
as.numeric(sim_probs[which(sim_probs$n == 0), "p"])
as.numeric(sim_probs[which(sim_probs$n == positives), "p"])
rbindlist(lapply(0:p, function(positives) { # number of positives
if (positives == 0) {
# prevalence_corr <-  1-((1-prevalence)*(1-(prevalence*tau))^(p-1))
data.frame(limit=lod, pool=p, pos=positives, prevalence=prevalence,
above.llod = above.lod[above],
concentration=0,
tau = tau,
tau_relative_var = TAU_RELATIVE_VAR,
n_eff = unique(sim_probs$n_eff),
probability = as.numeric(sim_probs[which(sim_probs$n == 0), "p"]),
#probability_subset1 =  sim_subset_probs[which(sim_subset_probs$n == 0), "p"],
probability_null_test = dbinom(positives, p, prevalence), # uncorrected prevalence
prevalence_corr = sim_probs$pi_eff[1],
probability_null =  sim_probs$prob_null_eff[positives+1], # prevalence corrected for correlated individuals
random=0,
z.index=0,
call.each.conc=FALSE, tests=1, tn=1,tp=0,fn=0,fp=0)
}
else {
dat <- matrix(sample(threshold.ct, positives * n, replace=T), nrow=positives)
# sample data uniformly at random
# n samples of positives, rearrange into a matrix of positive rows, n columns
each.conc = -log2(colSums(2^-dat)/p)+ifelse(dilution.vary.index==1,0,
rnorm(mean=0,sd=1.1,n=ncol(dat)))
# sd of 1.1 reflects confidence interval for deviation from perfect log2 dilution in assays
# calculation dilution based on number of positives (colSum) in total pool size (p)
z.index= probit.z.indices[probit.mode.index]
t = data.frame(
limit=lod, pool=p, pos=positives, prevalence=prevalence,
above.llod = above.lod[above],
concentration=0,
tau = tau,
tau_relative_var = TAU_RELATIVE_VAR,
n_eff = sim_probs$n_eff[1],
probability = as.numeric(sim_probs[which(sim_probs$n == positives), "p"]),
#probability_subset1 =  sim_subset_probs[which(sim_subset_probs$n == 0), "p"],
probability_null_test = dbinom(positives, p, prevalence), # uncorrected prevalence
prevalence_corr = sim_probs$pi_eff,
probability_null = sim_probs$prob_null_eff, # prevalence corrected for correlated individuals
random=sample(n,n,replace = TRUE)/n, #
z.index=ifelse(probit.mode.index<4,probit.z.indices[probit.mode.index],
sample(1:length(z_scores),n,replace=T))) %>%
mutate(
call.each.conc=probit[1+(z.index-1)*571+each.conc*10-(lod-35.9)*10,2]>random,
tests=1 + p * (call.each.conc), # number of tests done (number positive pools + pool size)
tn=0,
tp=1 * (call.each.conc),
fn=1 * (!call.each.conc),
fp=0)
return(t)
}
}))
t = data.frame(
limit=lod, pool=p, pos=positives, prevalence=prevalence,
above.llod = above.lod[above],
concentration=0,
tau = tau,
tau_relative_var = TAU_RELATIVE_VAR,
n_eff = sim_probs$n_eff[1],
probability = as.numeric(sim_probs[which(sim_probs$n == positives), "p"]),
#probability_subset1 =  sim_subset_probs[which(sim_subset_probs$n == 0), "p"],
probability_null_test = dbinom(positives, p, prevalence), # uncorrected prevalence
prevalence_corr = sim_probs$pi_eff,
probability_null = sim_probs$prob_null_eff, # prevalence corrected for correlated individuals
random=sample(n,n,replace = TRUE)/n, #
z.index=ifelse(probit.mode.index<4,probit.z.indices[probit.mode.index],
sample(1:length(z_scores),n,replace=T))) %>%
mutate(
call.each.conc=probit[1+(z.index-1)*571+each.conc*10-(lod-35.9)*10,2]>random,
tests=1 + p * (call.each.conc), # number of tests done (number positive pools + pool size)
tn=0,
tp=1 * (call.each.conc),
fn=1 * (!call.each.conc),
fp=0)
as.numeric(sim_probs[which(sim_probs$n == positives), "p"])
dbinom(positives, p, prevalence)
t = data.frame(
limit=lod, pool=p, pos=positives, prevalence=prevalence,
above.llod = above.lod[above],
concentration=0,
tau = tau,
tau_relative_var = TAU_RELATIVE_VAR,
n_eff = sim_probs$n_eff[1],
probability = as.numeric(sim_probs[which(sim_probs$n == positives), "p"]),
#probability_subset1 =  sim_subset_probs[which(sim_subset_probs$n == 0), "p"],
probability_null_test = dbinom(positives, p, prevalence), # uncorrected prevalence
prevalence_corr = sim_probs$pi_eff[1],
probability_null = sim_probs$prob_null_eff[positives+1], # prevalence corrected for correlated individuals
random=sample(n,n,replace = TRUE)/n, #
z.index=ifelse(probit.mode.index<4,probit.z.indices[probit.mode.index],
sample(1:length(z_scores),n,replace=T))) %>%
mutate(
call.each.conc=probit[1+(z.index-1)*571+each.conc*10-(lod-35.9)*10,2]>random,
tests=1 + p * (call.each.conc), # number of tests done (number positive pools + pool size)
tn=0,
tp=1 * (call.each.conc),
fn=1 * (!call.each.conc),
fp=0)
rbindlist(lapply(0:p, function(positives) { # number of positives
if (positives == 0) {
# prevalence_corr <-  1-((1-prevalence)*(1-(prevalence*tau))^(p-1))
data.frame(limit=lod, pool=p, pos=positives, prevalence=prevalence,
above.llod = above.lod[above],
concentration=0,
tau = tau,
tau_relative_var = TAU_RELATIVE_VAR,
n_eff = unique(sim_probs$n_eff),
probability = as.numeric(sim_probs[which(sim_probs$n == 0), "p"]),
#probability_subset1 =  sim_subset_probs[which(sim_subset_probs$n == 0), "p"],
probability_null_test = dbinom(positives, p, prevalence), # uncorrected prevalence
prevalence_corr = sim_probs$pi_eff[1],
probability_null =  sim_probs$prob_null_eff[positives+1], # prevalence corrected for correlated individuals
random=0,
z.index=0,
call.each.conc=FALSE, tests=1, tn=1,tp=0,fn=0,fp=0)
}
else {
dat <- matrix(sample(threshold.ct, positives * n, replace=T), nrow=positives)
# sample data uniformly at random
# n samples of positives, rearrange into a matrix of positive rows, n columns
each.conc = -log2(colSums(2^-dat)/p)+ifelse(dilution.vary.index==1,0,
rnorm(mean=0,sd=1.1,n=ncol(dat)))
# sd of 1.1 reflects confidence interval for deviation from perfect log2 dilution in assays
# calculation dilution based on number of positives (colSum) in total pool size (p)
z.index= probit.z.indices[probit.mode.index]
t = data.frame(
limit=lod, pool=p, pos=positives, prevalence=prevalence,
above.llod = above.lod[above],
concentration=0,
tau = tau,
tau_relative_var = TAU_RELATIVE_VAR,
n_eff = sim_probs$n_eff[1],
probability = as.numeric(sim_probs[which(sim_probs$n == positives), "p"]),
#probability_subset1 =  sim_subset_probs[which(sim_subset_probs$n == 0), "p"],
probability_null_test = dbinom(positives, p, prevalence), # uncorrected prevalence
prevalence_corr = sim_probs$pi_eff[1],
probability_null = sim_probs$prob_null_eff[positives+1], # prevalence corrected for correlated individuals
random=sample(n,n,replace = TRUE)/n, #
z.index=ifelse(probit.mode.index<4,probit.z.indices[probit.mode.index],
sample(1:length(z_scores),n,replace=T))) %>%
mutate(
call.each.conc=probit[1+(z.index-1)*571+each.conc*10-(lod-35.9)*10,2]>random,
tests=1 + p * (call.each.conc), # number of tests done (number positive pools + pool size)
tn=0,
tp=1 * (call.each.conc),
fn=1 * (!call.each.conc),
fp=0)
return(t)
}
}))
allfirst.poolct <- experiment_loop(above, ct_fake_input, p, prevalence, tau) # just for 15% Ct > LoD
experiment_loop <- function(above, ct_dat, p, prevalence, tau){
ct_set <- pmax(5,subset(ct_dat + mat2[above,3], ct_dat + mat2[above,3]<45))
# (1) generate additional set of C_t values with above = 5-30% of values above each LoD; subset to only valid Ct values (<45)
# (2) ct_set is the parallel maximum of the C_t values from (1) vs. 5 (why 5??)
threshold.ct <- c(sample(ct_set, n, replace=T)) # sample ct_set with replacement # prevalence ("proportion positive tests")
print(prevalence)
print(tau)
sim_probs <- compute_probas_level2_variation(p, prevalence, tau, TAU_RELATIVE_VAR)
rbindlist(lapply(0:p, function(positives) { # number of positives
print(positives)
if (positives == 0) {
# prevalence_corr <-  1-((1-prevalence)*(1-(prevalence*tau))^(p-1))
data.frame(limit=lod, pool=p, pos=positives, prevalence=prevalence,
above.llod = above.lod[above],
concentration=0,
tau = tau,
tau_relative_var = TAU_RELATIVE_VAR,
n_eff = unique(sim_probs$n_eff),
probability = as.numeric(sim_probs[which(sim_probs$n == 0), "p"]),
#probability_subset1 =  sim_subset_probs[which(sim_subset_probs$n == 0), "p"],
probability_null_test = dbinom(positives, p, prevalence), # uncorrected prevalence
prevalence_corr = sim_probs$pi_eff[1],
probability_null =  sim_probs$prob_null_eff[positives+1], # prevalence corrected for correlated individuals
random=0,
z.index=0,
call.each.conc=FALSE, tests=1, tn=1,tp=0,fn=0,fp=0)
}
else {
dat <- matrix(sample(threshold.ct, positives * n, replace=T), nrow=positives)
# sample data uniformly at random
# n samples of positives, rearrange into a matrix of positive rows, n columns
each.conc = -log2(colSums(2^-dat)/p)+ifelse(dilution.vary.index==1,0,
rnorm(mean=0,sd=1.1,n=ncol(dat)))
# sd of 1.1 reflects confidence interval for deviation from perfect log2 dilution in assays
# calculation dilution based on number of positives (colSum) in total pool size (p)
z.index= probit.z.indices[probit.mode.index]
t = data.frame(
limit=lod, pool=p, pos=positives, prevalence=prevalence,
above.llod = above.lod[above],
concentration=0,
tau = tau,
tau_relative_var = TAU_RELATIVE_VAR,
n_eff = sim_probs$n_eff[1],
probability = as.numeric(sim_probs[which(sim_probs$n == positives), "p"]),
#probability_subset1 =  sim_subset_probs[which(sim_subset_probs$n == 0), "p"],
probability_null_test = dbinom(positives, p, prevalence), # uncorrected prevalence
prevalence_corr = sim_probs$pi_eff[1],
probability_null = sim_probs$prob_null_eff[positives+1], # prevalence corrected for correlated individuals
random=sample(n,n,replace = TRUE)/n, #
z.index=ifelse(probit.mode.index<4,probit.z.indices[probit.mode.index],
sample(1:length(z_scores),n,replace=T))) %>%
mutate(
call.each.conc=probit[1+(z.index-1)*571+each.conc*10-(lod-35.9)*10,2]>random,
tests=1 + p * (call.each.conc), # number of tests done (number positive pools + pool size)
tn=0,
tp=1 * (call.each.conc),
fn=1 * (!call.each.conc),
fp=0)
return(t)
}
}))
}
allfirst.poolct <- experiment_loop(above, ct_fake_input, p, prevalence, tau) # just for 15% Ct > LoD
group_by(allfirst.poolct, pool, prevalence, above.llod, limit, tau) %>%
summarize(pos1=weighted.mean(pos, w=probability),
pos_null=weighted.mean(pos, w=probability_null),
total.tests=weighted.mean(tests, w=probability),
total.tests_null=weighted.mean(tests, w=probability_null),
tests.per.sample=weighted.mean(tests, w=probability)/mean(pool), # calculate tests per sample
tests.per.sample_null=weighted.mean(tests, w=probability_null)/mean(pool),
tn1=weighted.mean(tn, w=probability),
tp1=weighted.mean(tp, w=probability),
fn1=weighted.mean(fn, w=probability),
fp1=weighted.mean(fp, w=probability),
ppa = weighted.mean(tp/(fn + tp), w=probability, na.rm = TRUE),
ppa_null = weighted.mean(tp/(fn + tp), w=probability_null, na.rm = TRUE),
tn_null=weighted.mean(tn, w=probability_null),
tp_null=weighted.mean(tp, w=probability_null),
fn_null=weighted.mean(fn, w=probability_null),
fp_null=weighted.mean(fp, w=probability_null)) -> apw
apw
a = 0;for (var_tau in seq(from=1, to=6, by=0.5) ){
print(c(prev,tau))
start_time <- Sys.time()
res =rbind(lapply(1:300, function(it)
{data.frame(p = compute_p_all_levels(N, runif(N, prev*0.5 ,prev*1.5), tau,
var_tau=var_tau, B=1000),
prev = prev,
tau=tau,
var = var_tau,
it = it)}))
if (a==0){
res_tot  = res
}else{
res_tot = rbind(res_tot, res)
}
a  = a+1
end_time <- Sys.time()
print(end_time - start_time)
}
prev = 0.01
tau
N=15
compute_p_all_levels <- function(N, prevs, tau,var_tau=2, B=100000){
#### when both prev and tau are vectors
probs <- factor(apply(sapply(prevs, function(x){rbinom(B,1,x)}),1,
function(x){sum(x[-sample(1:N,1)])}), levels= 0:(N-1))
probs = as.data.frame(table(probs))
probs$Freq = probs$Freq/B
probs$probs = as.numeric(probs$probs)
return(
sum(apply(sapply(1:B, function(b){
sapply(1:(N-1), function(K){
(1-exp(sum(log(1- 1/(1+exp(-rnorm(K,log(tau/(1-tau)), log(var_tau)/2)))))))})}),1,mean) *probs$Freq[2:N])* (1-mean(prevs)) + mean(prevs)
)
# return(
#   sum(apply(sapply(1:B, function(b){
#     sapply(1:(N-1), function(K){
#       (1-exp(sum(log(1- 1/(1+exp(-rnorm(K,log(tau/(1-tau)), log(var_tau)/2)))))))})}),1,mean) *probs$Freq[2:N])* (1-mean(prevs)) + mean(prevs)
# )
}
for (var_tau in seq(from=1, to=6, by=0.5) ){
print(c(prev,tau))
start_time <- Sys.time()
res =rbind(lapply(1:300, function(it)
{data.frame(p = compute_p_all_levels(N, runif(N, prev*0.5 ,prev*1.5), tau,
var_tau=var_tau, B=1000),
prev = prev,
tau=tau,
var = var_tau,
it = it)}))
if (a==0){
res_tot  = res
}else{
res_tot = rbind(res_tot, res)
}
a  = a+1
end_time <- Sys.time()
print(end_time - start_time)
}
res
res_tot
compute_p_all_levels <- function(N, prevs, tau,var_tau=2, B=100000){
#### when both prev and tau are vectors
probs <- factor(apply(sapply(prevs, function(x){rbinom(B,1,x)}),1,
function(x){sum(x[-sample(1:N,1)])}), levels= 0:(N-1))
probs = as.data.frame(table(probs))
probs$Freq = probs$Freq/B
probs$probs = as.numeric(probs$probs)
return(
sum(apply(sapply(1:B, function(b){
sapply(1:(N-1), function(K){
(1-exp(sum(log(1- 1/(1+exp(-rnorm(K,log(tau/(1-tau)), log(var_tau)/2)))))))})}),1,mean) *probs$Freq[2:N])* (1-mean(prevs)) + mean(prevs)
)
# return(
#   sum(apply(sapply(1:B, function(b){
#     sapply(1:(N-1), function(K){
#       (1-exp(sum(log(1- 1/(1+exp(-rnorm(K,log(tau/(1-tau)), log(var_tau)/2)))))))})}),1,mean) *probs$Freq[2:N])* (1-mean(prevs)) + mean(prevs)
# )
}
for (var_tau in seq(from=1, to=6, by=0.5) ){
print(c(prev,tau))
start_time <- Sys.time()
res =rbindlist(lapply(1:300, function(it)
{data.frame(p = compute_p_all_levels(N, runif(N, prev*0.5 ,prev*1.5), tau,
var_tau=var_tau, B=1000),
prev = prev,
tau=tau,
var = var_tau,
it = it)}))
if (a==0){
res_tot  = res
}else{
res_tot = rbind(res_tot, res)
}
a  = a+1
end_time <- Sys.time()
print(end_time - start_time)
}
res
res_tot
res
res2 = res
rbind(res, res2)
res
a
a=0;compute_p_all_levels <- function(N, prevs, tau,var_tau=2, B=100000){
#### when both prev and tau are vectors
probs <- factor(apply(sapply(prevs, function(x){rbinom(B,1,x)}),1,
function(x){sum(x[-sample(1:N,1)])}), levels= 0:(N-1))
probs = as.data.frame(table(probs))
probs$Freq = probs$Freq/B
probs$probs = as.numeric(probs$probs)
return(
sum(apply(sapply(1:B, function(b){
sapply(1:(N-1), function(K){
(1-exp(sum(log(1- 1/(1+exp(-rnorm(K,log(tau/(1-tau)), log(var_tau)/2)))))))})}),1,mean) *probs$Freq[2:N])* (1-mean(prevs)) + mean(prevs)
)
# return(
#   sum(apply(sapply(1:B, function(b){
#     sapply(1:(N-1), function(K){
#       (1-exp(sum(log(1- 1/(1+exp(-rnorm(K,log(tau/(1-tau)), log(var_tau)/2)))))))})}),1,mean) *probs$Freq[2:N])* (1-mean(prevs)) + mean(prevs)
# )
}
for (var_tau in seq(from=1, to=6, by=0.5) ){
print(c(prev,tau))
start_time <- Sys.time()
res =rbindlist(lapply(1:300, function(it)
{data.frame(p = compute_p_all_levels(N, runif(N, prev*0.5 ,prev*1.5), tau,
var_tau=var_tau, B=1000),
prev = prev,
tau=tau,
var = var_tau,
it = it)}))
if (a==0){
res_tot  = res
}else{
res_tot = rbind(res_tot, res)
}
a  = a+1
end_time <- Sys.time()
print(end_time - start_time)
}
res_tot

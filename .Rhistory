#dev.off()
library(ggpubr)
#pdf("plot_test.pdf",width=8, height=5)
print(ggarrange(ppa_plot, tests_plot,
labels = c("A", "B"),
ncol=1, nrow = 2,
common.legend = TRUE, legend = "right"))
#dev.off()
p=10
prevalence=0.1
tau=0.1
sim_probs <- compute_probas(p, prevalence, tau)
n_eff <- ceiling(compute_neff(p, p, prevalence, tau))
prev_eff <- compute_p(p, prevalence, tau)
n_eff
prev_eff
# Model loop, vary % above each LOD (simplified from original code)
# source("MCMC_sim.R")
# contains calc_probs() fxn. This SIMULATES the number of positives per pool
source("compute_probas.R")
# contains exact_probs fxn. This CALCULATES the number of positives per pool
pool.max = 20
# if you want to use the simulated probabilities that don't account for community infections (network only)
# use this to get model_output_corr_indiv_.1.2.csv from experiment_loop()
# sim_probs_.1.2 <- read.csv("sim_probs_.1.2.csv")
# sim_probs_.1.2[,1] <- NULL
experiment_loop <- function(above, ct_dat, sim_min, sim_max){
ct_set <- pmax(5,subset(ct_dat + mat2[above,3], ct_dat + mat2[above,3]<45))
# (1) generate additional set of C_t values with above = 5-30% of values above each LoD; subset to only valid Ct values (<45)
# (2) ct_set is the parallel maximum of the C_t values from (1) vs. 5 (why 5??)
threshold.ct <- c(sample(ct_set, n, replace=T)) # sample ct_set with replacement
rbindlist(lapply(1:pool.max, function (p) { # pool size
rbindlist(lapply(c(0.001,0.01, 0.03, 0.05,0.1), function(prevalence) { # prevalence ("proportion positive tests")
# sim_probs <- calc_probs_all_homogeneous(p, prevalence, min=sim_min, max=sim_max, B=1000)
# specify range of correlation to sample from (min, max)
rbindlist(lapply(c(0.1, 0.2, 0.3, 0.4, 0.5), function(tau) {
sim_probs <- compute_probas(p, prevalence, tau)
n_eff <- ceiling(compute_neff(p, p, prevalence, tau))
prev_eff <- compute_p(p, prevalence, tau)
#sim_subset_probs <- compute_probas_subset(50, p, prevalence, tau)
# sim_probs <- exact_probs(group_size = 1, tau = .2, pop_prev = .1)
rbindlist(lapply(0:p, function(positives) { # number of positives
#print(positives)
if (positives == 0) {
# prevalence_corr <-  1-((1-prevalence)*(1-(prevalence*tau))^(p-1))
data.frame(limit=lod, pool=p, pos=positives, prevalence=prevalence,
above.llod = above.lod[above],
concentration=0,
tau = tau,
n_eff = n_eff,
probability = sim_probs[which(sim_probs$n == 0), "p"],
#probability_subset1 =  sim_subset_probs[which(sim_subset_probs$n == 0), "p"],
probability_null_test = dbinom(positives, p, prevalence), # uncorrected prevalence
prevalence_corr =prev_eff,
probability_null = dbinom(positives, p, prev_eff), # prevalence corrected for correlated individuals
random=0, z.index=0,
call.each.conc=FALSE, tests=1, tn=1,tp=0,fn=0,fp=0)
}
else {
dat <- matrix(sample(threshold.ct, positives * n, replace=T), nrow=positives)
# sample data uniformly at random
# n samples of positives, rearrange into a matrix of positive rows, n columns
each.conc = -log2(colSums(2^-dat)/p)+ifelse(dilution.vary.index==1,0,
rnorm(mean=0,sd=1.1,n=ncol(dat)))
# sd of 1.1 reflects confidence interval for deviation from perfect log2 dilution in assays
# calculation dilution based on number of positives (colSum) in total pool size (p)
data.frame(
limit=lod,
pool=p,
pos=positives,
prevalence=prevalence,
above.llod=above.lod[above],
concentration=each.conc,
tau = tau,
n_eff = n_eff,
# probability = (1-(1-prevalence)^p)*sim_probs_.1.2[positives, (p-1)], # using network only infection probs
probability = sim_probs[which(sim_probs$n == positives), "p"]/n,
#probability_subset1 =  sim_subset_probs[which(sim_subset_probs$n == positives), "p"]/n,
probability_null_test = dbinom(positives, p, prevalence)/n,
prevalence_corr = prev_eff,
probability_null = dbinom(positives, p, prev_eff)/n,
random=sample(n,n,replace = TRUE)/n, #
z.index=ifelse(probit.mode.index<4,probit.z.indices[probit.mode.index],
sample(1:length(z_scores),n,replace=T))) %>%
mutate(
call.each.conc=probit[1+(z.index-1)*571+each.conc*10-(lod-35.9)*10,2]>random,
tests=1 + p * (call.each.conc), # number of tests done (number positive pools + pool size)
tn=0,
tp=1 * (call.each.conc),
fn=1 * (!call.each.conc),
fp=0)
}
}))
}))
}))
}))
}
# apply the model loop to the fake correlated data
#allfirst.poolct <- rbindlist(lapply(1:length(above.lod), experiment_loop, sim_min = 0.1, sim_max = 0.2, ct_dat = ct_fake_input))
allfirst.poolct <- rbindlist(lapply(3, experiment_loop, ct_dat = ct_fake_input)) # just for 15% Ct > LoD
group_by(allfirst.poolct, pool, prevalence, above.llod, limit, tau) %>%
summarize(pos1=weighted.mean(pos, w=probability),
pos_null=weighted.mean(pos, w=probability_null),
total.tests=weighted.mean(tests, w=probability),
total.tests_null=weighted.mean(tests, w=probability_null),
tests.per.sample=weighted.mean(tests, w=probability)/mean(pool), # calculate tests per sample
tests.per.sample_null=weighted.mean(tests, w=probability_null)/mean(pool),
tn1=weighted.mean(tn, w=probability),
tp1=weighted.mean(tp, w=probability),
fn1=weighted.mean(fn, w=probability),
fp1=weighted.mean(fp, w=probability),
ppa = weighted.mean(tp/(fn + tp), w=probability, na.rm = TRUE),
ppa_null = weighted.mean(tp/(fn + tp), w=probability_null, na.rm = TRUE),
tn_null=weighted.mean(tn, w=probability_null),
tp_null=weighted.mean(tp, w=probability_null),
fn_null=weighted.mean(fn, w=probability_null),
fp_null=weighted.mean(fp, w=probability_null),
ppa_null_observed = weighted.mean(tp/(fn + tp), w=probability_null_test, na.rm = TRUE),
tests.per.sample_null_observed=weighted.mean(tests, w=probability_null_test)/mean(pool),
tn_null_observed=weighted.mean(tn, w=probability_null_test),
tp_null_observed=weighted.mean(tp, w=probability_null_test),
fn_null_observed =weighted.mean(fn, w=probability_null_test),
fp_null_observed =weighted.mean(fp, w=probability_null_test)) -> apw
group_by(allfirst.poolct, pool, prevalence, above.llod, limit, tau) %>%
summarize(n_eff = mean(n_eff),
pos1=weighted.mean(pos, w=probability),
pos_null=weighted.mean(pos, w=probability_null),
total.tests=weighted.mean(tests, w=probability),
total.tests_null=weighted.mean(tests, w=probability_null),
tests.per.sample=weighted.mean(tests, w=probability)/mean(pool), # calculate tests per sample
tests.per.sample_null=weighted.mean(tests, w=probability_null)/mean(pool),
tn1=weighted.mean(tn, w=probability),
tp1=weighted.mean(tp, w=probability),
fn1=weighted.mean(fn, w=probability),
fp1=weighted.mean(fp, w=probability),
ppa = weighted.mean(tp/(fn + tp), w=probability, na.rm = TRUE),
ppa_null = weighted.mean(tp/(fn + tp), w=probability_null, na.rm = TRUE),
tn_null=weighted.mean(tn, w=probability_null),
tp_null=weighted.mean(tp, w=probability_null),
fn_null=weighted.mean(fn, w=probability_null),
fp_null=weighted.mean(fp, w=probability_null),
ppa_null_observed = weighted.mean(tp/(fn + tp), w=probability_null_test, na.rm = TRUE),
tests.per.sample_null_observed=weighted.mean(tests, w=probability_null_test)/mean(pool),
tn_null_observed=weighted.mean(tn, w=probability_null_test),
tp_null_observed=weighted.mean(tp, w=probability_null_test),
fn_null_observed =weighted.mean(fn, w=probability_null_test),
fp_null_observed =weighted.mean(fp, w=probability_null_test)) -> apw
# rm(allfirst.poolct)
all <- as.data.frame(apw)
#allci <- read.csv("correlated_exactprobs_Ct15.csv")
ppa_plot <- ggplot() +
geom_line(allci, mapping = aes(x=pool, y= ppa, color=as.factor(prevalence),linetype="correlated")) +
geom_line(allci, mapping = aes(x=n_eff, y=ppa_null, color=as.factor(prevalence),linetype="null")) +
facet_grid(cols = vars(factor(allci$tau, labels = c("Tau = 0.1", "0.2", "0.3", "0.4", "0.5")))) +
theme_bw() +
theme(axis.title.x = element_blank())+
scale_color_brewer(palette="RdYlBu", name="Prevalence") +
# xlab("Pool size") +
ylab("Expected PPA") +
# ggtitle("Correlated vs. IID Samples") +
scale_x_continuous(limits=c(1,20), breaks=seq(0,20,4)) + scale_y_continuous(limits=c(0.8,1),breaks=seq(0.5,1,.1)) +
guides(color=guide_legend(title="Community Prevalence")) +
guides(linetype=guide_legend(title="Correlation Structure"))
# Print plot of pool size vs. average tests/sample, panel grid of %Ct > LoD, color by proportion test pos
#pdf("plot_test.pdf",width=8, height=5)
tests_plot <- ggplot() +
geom_line(allci, mapping = aes(x=pool, y= tests.per.sample, color=as.factor(prevalence), linetype = "correlated")) +
geom_line(allci, mapping = aes(x=n_eff, y= tests.per.sample_null, color=as.factor(prevalence), linetype = "null")) +
facet_grid(cols = vars(factor(allci$tau))) +
theme_bw() +
scale_color_brewer(palette="RdYlBu", name="Prevalence") +
xlab("Pool size") + ylab("Average tests per sample") +
scale_x_continuous(limits=c(1,20), breaks=seq(0,20,4)) + scale_y_continuous(trans="reverse", limits=c(1.2,0),breaks=seq(0,1.2,0.2)) +
guides(color=guide_legend(title="Community Prevalence")) +
guides(linetype=guide_legend(title="Correlation Structure"))
#dev.off()
library(ggpubr)
#pdf("plot_test.pdf",width=8, height=5)
print(ggarrange(ppa_plot, tests_plot,
labels = c("A", "B"),
ncol=1, nrow = 2,
common.legend = TRUE, legend = "right"))
#dev.off()
all_ci$n_eff
allci$n_eff
all <- as.data.frame(apw)
cpi <- as_tibble(BinomCI(all$ppa*n,n,conf.level=0.95,method="clopper-pearson"))
allci <- bind_cols(all,cpi)
allci$n_eff]
allci$n_eff
#allci <- read.csv("correlated_exactprobs_Ct15.csv")
ppa_plot <- ggplot() +
geom_line(allci, mapping = aes(x=pool, y= ppa, color=as.factor(prevalence),linetype="correlated")) +
geom_line(allci, mapping = aes(x=n_eff, y=ppa_null, color=as.factor(prevalence),linetype="null")) +
facet_grid(cols = vars(factor(allci$tau, labels = c("Tau = 0.1", "0.2", "0.3", "0.4", "0.5")))) +
theme_bw() +
theme(axis.title.x = element_blank())+
scale_color_brewer(palette="RdYlBu", name="Prevalence") +
# xlab("Pool size") +
ylab("Expected PPA") +
# ggtitle("Correlated vs. IID Samples") +
scale_x_continuous(limits=c(1,20), breaks=seq(0,20,4)) + scale_y_continuous(limits=c(0.8,1),breaks=seq(0.5,1,.1)) +
guides(color=guide_legend(title="Community Prevalence")) +
guides(linetype=guide_legend(title="Correlation Structure"))
# Print plot of pool size vs. average tests/sample, panel grid of %Ct > LoD, color by proportion test pos
#pdf("plot_test.pdf",width=8, height=5)
tests_plot <- ggplot() +
geom_line(allci, mapping = aes(x=pool, y= tests.per.sample, color=as.factor(prevalence), linetype = "correlated")) +
geom_line(allci, mapping = aes(x=n_eff, y= tests.per.sample_null, color=as.factor(prevalence), linetype = "null")) +
facet_grid(cols = vars(factor(allci$tau))) +
theme_bw() +
scale_color_brewer(palette="RdYlBu", name="Prevalence") +
xlab("Pool size") + ylab("Average tests per sample") +
scale_x_continuous(limits=c(1,20), breaks=seq(0,20,4)) + scale_y_continuous(trans="reverse", limits=c(1.2,0),breaks=seq(0,1.2,0.2)) +
guides(color=guide_legend(title="Community Prevalence")) +
guides(linetype=guide_legend(title="Correlation Structure"))
#dev.off()
library(ggpubr)
#pdf("plot_test.pdf",width=8, height=5)
print(ggarrange(ppa_plot, tests_plot,
labels = c("A", "B"),
ncol=1, nrow = 2,
common.legend = TRUE, legend = "right"))
#dev.off()
#allci <- read.csv("correlated_exactprobs_Ct15.csv")
ppa_plot <- ggplot() +
geom_line(allci, mapping = aes(x=n_eff, y= ppa, color=as.factor(prevalence),linetype="correlated")) +
geom_line(allci, mapping = aes(x=pool, y=ppa_null, color=as.factor(prevalence),linetype="null")) +
facet_grid(cols = vars(factor(allci$tau, labels = c("Tau = 0.1", "0.2", "0.3", "0.4", "0.5")))) +
theme_bw() +
theme(axis.title.x = element_blank())+
scale_color_brewer(palette="RdYlBu", name="Prevalence") +
# xlab("Pool size") +
ylab("Expected PPA") +
# ggtitle("Correlated vs. IID Samples") +
scale_x_continuous(limits=c(1,20), breaks=seq(0,20,4)) + scale_y_continuous(limits=c(0.8,1),breaks=seq(0.5,1,.1)) +
guides(color=guide_legend(title="Community Prevalence")) +
guides(linetype=guide_legend(title="Correlation Structure"))
# Print plot of pool size vs. average tests/sample, panel grid of %Ct > LoD, color by proportion test pos
#pdf("plot_test.pdf",width=8, height=5)
tests_plot <- ggplot() +
geom_line(allci, mapping = aes(x=n_eff, y= tests.per.sample, color=as.factor(prevalence), linetype = "correlated")) +
geom_line(allci, mapping = aes(x=pool, y= tests.per.sample_null, color=as.factor(prevalence), linetype = "null")) +
facet_grid(cols = vars(factor(allci$tau))) +
theme_bw() +
scale_color_brewer(palette="RdYlBu", name="Prevalence") +
xlab("Pool size") + ylab("Average tests per sample") +
scale_x_continuous(limits=c(1,20), breaks=seq(0,20,4)) + scale_y_continuous(trans="reverse", limits=c(1.2,0),breaks=seq(0,1.2,0.2)) +
guides(color=guide_legend(title="Community Prevalence")) +
guides(linetype=guide_legend(title="Correlation Structure"))
#dev.off()
library(ggpubr)
#pdf("plot_test.pdf",width=8, height=5)
print(ggarrange(ppa_plot, tests_plot,
labels = c("A", "B"),
ncol=1, nrow = 2,
common.legend = TRUE, legend = "right"))
#dev.off()
# Model loop, vary % above each LOD (simplified from original code)
# source("MCMC_sim.R")
# contains calc_probs() fxn. This SIMULATES the number of positives per pool
source("compute_probas.R")
# contains exact_probs fxn. This CALCULATES the number of positives per pool
pool.max = 20
# if you want to use the simulated probabilities that don't account for community infections (network only)
# use this to get model_output_corr_indiv_.1.2.csv from experiment_loop()
# sim_probs_.1.2 <- read.csv("sim_probs_.1.2.csv")
# sim_probs_.1.2[,1] <- NULL
experiment_loop <- function(above, ct_dat, sim_min, sim_max){
ct_set <- pmax(5,subset(ct_dat + mat2[above,3], ct_dat + mat2[above,3]<45))
# (1) generate additional set of C_t values with above = 5-30% of values above each LoD; subset to only valid Ct values (<45)
# (2) ct_set is the parallel maximum of the C_t values from (1) vs. 5 (why 5??)
threshold.ct <- c(sample(ct_set, n, replace=T)) # sample ct_set with replacement
rbindlist(lapply(1:pool.max, function (p) { # pool size
rbindlist(lapply(c(0.0001,0.001,0.005, 0.01, 0.03, 0.05, 0.1, 0.2), function(prevalence) { # prevalence ("proportion positive tests")
# sim_probs <- calc_probs_all_homogeneous(p, prevalence, min=sim_min, max=sim_max, B=1000)
# specify range of correlation to sample from (min, max)
rbindlist(lapply(c(0, 0.001, 0.01, 0.05, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7), function(tau) {
sim_probs <- compute_probas(p, prevalence, tau)
n_eff <- compute_neff(p, p, prevalence, tau)
prev_eff <- compute_p(p, prevalence, tau)
#sim_subset_probs <- compute_probas_subset(50, p, prevalence, tau)
# sim_probs <- exact_probs(group_size = 1, tau = .2, pop_prev = .1)
rbindlist(lapply(0:p, function(positives) { # number of positives
#print(positives)
if (positives == 0) {
# prevalence_corr <-  1-((1-prevalence)*(1-(prevalence*tau))^(p-1))
data.frame(limit=lod, pool=p, pos=positives, prevalence=prevalence,
above.llod = above.lod[above],
concentration=0,
tau = tau,
n_eff = n_eff,
probability = sim_probs[which(sim_probs$n == 0), "p"],
#probability_subset1 =  sim_subset_probs[which(sim_subset_probs$n == 0), "p"],
probability_null_test = dbinom(positives, p, prevalence), # uncorrected prevalence
prevalence_corr =prev_eff,
probability_null = dbinom(positives, p, prev_eff), # prevalence corrected for correlated individuals
random=0, z.index=0,
call.each.conc=FALSE, tests=1, tn=1,tp=0,fn=0,fp=0)
}
else {
dat <- matrix(sample(threshold.ct, positives * n, replace=T), nrow=positives)
# sample data uniformly at random
# n samples of positives, rearrange into a matrix of positive rows, n columns
each.conc = -log2(colSums(2^-dat)/p)+ifelse(dilution.vary.index==1,0,
rnorm(mean=0,sd=1.1,n=ncol(dat)))
# sd of 1.1 reflects confidence interval for deviation from perfect log2 dilution in assays
# calculation dilution based on number of positives (colSum) in total pool size (p)
data.frame(
limit=lod,
pool=p,
pos=positives,
prevalence=prevalence,
above.llod=above.lod[above],
concentration=each.conc,
tau = tau,
n_eff = n_eff,
# probability = (1-(1-prevalence)^p)*sim_probs_.1.2[positives, (p-1)], # using network only infection probs
probability = sim_probs[which(sim_probs$n == positives), "p"]/n,
#probability_subset1 =  sim_subset_probs[which(sim_subset_probs$n == positives), "p"]/n,
probability_null_test = dbinom(positives, p, prevalence)/n,
prevalence_corr = prev_eff,
probability_null = dbinom(positives, p, prev_eff)/n,
random=sample(n,n,replace = TRUE)/n, #
z.index=ifelse(probit.mode.index<4,probit.z.indices[probit.mode.index],
sample(1:length(z_scores),n,replace=T))) %>%
mutate(
call.each.conc=probit[1+(z.index-1)*571+each.conc*10-(lod-35.9)*10,2]>random,
tests=1 + p * (call.each.conc), # number of tests done (number positive pools + pool size)
tn=0,
tp=1 * (call.each.conc),
fn=1 * (!call.each.conc),
fp=0)
}
}))
}))
}))
}))
}
# apply the model loop to the fake correlated data
#allfirst.poolct <- rbindlist(lapply(1:length(above.lod), experiment_loop, sim_min = 0.1, sim_max = 0.2, ct_dat = ct_fake_input))
allfirst.poolct <- rbindlist(lapply(3, experiment_loop, ct_dat = ct_fake_input)) # just for 15% Ct > LoD
rm(allci)
rm(allfirst.poolct)
# Model loop, vary % above each LOD (simplified from original code)
# source("MCMC_sim.R")
# contains calc_probs() fxn. This SIMULATES the number of positives per pool
source("compute_probas.R")
# contains exact_probs fxn. This CALCULATES the number of positives per pool
pool.max = 20
# if you want to use the simulated probabilities that don't account for community infections (network only)
# use this to get model_output_corr_indiv_.1.2.csv from experiment_loop()
# sim_probs_.1.2 <- read.csv("sim_probs_.1.2.csv")
# sim_probs_.1.2[,1] <- NULL
experiment_loop <- function(above, ct_dat, sim_min, sim_max){
ct_set <- pmax(5,subset(ct_dat + mat2[above,3], ct_dat + mat2[above,3]<45))
# (1) generate additional set of C_t values with above = 5-30% of values above each LoD; subset to only valid Ct values (<45)
# (2) ct_set is the parallel maximum of the C_t values from (1) vs. 5 (why 5??)
threshold.ct <- c(sample(ct_set, n, replace=T)) # sample ct_set with replacement
rbindlist(lapply(1:pool.max, function (p) { # pool size
rbindlist(lapply(c(0.0001,0.001,0.005, 0.01, 0.03, 0.05, 0.1, 0.2), function(prevalence) { # prevalence ("proportion positive tests")
# sim_probs <- calc_probs_all_homogeneous(p, prevalence, min=sim_min, max=sim_max, B=1000)
# specify range of correlation to sample from (min, max)
rbindlist(lapply(c(0, 0.001, 0.01, 0.05, 0.1, 0.2, 0.3, 0.4, 0.5,  0.7), function(tau) {
sim_probs <- compute_probas(p, prevalence, tau)
n_eff <- compute_neff(p, p, prevalence, tau)
prev_eff <- compute_p(p, prevalence, tau)
#sim_subset_probs <- compute_probas_subset(50, p, prevalence, tau)
# sim_probs <- exact_probs(group_size = 1, tau = .2, pop_prev = .1)
rbindlist(lapply(0:p, function(positives) { # number of positives
#print(positives)
if (positives == 0) {
# prevalence_corr <-  1-((1-prevalence)*(1-(prevalence*tau))^(p-1))
data.frame(limit=lod, pool=p, pos=positives, prevalence=prevalence,
above.llod = above.lod[above],
concentration=0,
tau = tau,
n_eff = n_eff,
probability = sim_probs[which(sim_probs$n == 0), "p"],
#probability_subset1 =  sim_subset_probs[which(sim_subset_probs$n == 0), "p"],
probability_null_test = dbinom(positives, p, prevalence), # uncorrected prevalence
prevalence_corr =prev_eff,
probability_null = dbinom(positives, p, prev_eff), # prevalence corrected for correlated individuals
random=0, z.index=0,
call.each.conc=FALSE, tests=1, tn=1,tp=0,fn=0,fp=0)
}
else {
dat <- matrix(sample(threshold.ct, positives * n, replace=T), nrow=positives)
# sample data uniformly at random
# n samples of positives, rearrange into a matrix of positive rows, n columns
each.conc = -log2(colSums(2^-dat)/p)+ifelse(dilution.vary.index==1,0,
rnorm(mean=0,sd=1.1,n=ncol(dat)))
# sd of 1.1 reflects confidence interval for deviation from perfect log2 dilution in assays
# calculation dilution based on number of positives (colSum) in total pool size (p)
data.frame(
limit=lod,
pool=p,
pos=positives,
prevalence=prevalence,
above.llod=above.lod[above],
concentration=each.conc,
tau = tau,
n_eff = n_eff,
# probability = (1-(1-prevalence)^p)*sim_probs_.1.2[positives, (p-1)], # using network only infection probs
probability = sim_probs[which(sim_probs$n == positives), "p"]/n,
#probability_subset1 =  sim_subset_probs[which(sim_subset_probs$n == positives), "p"]/n,
probability_null_test = dbinom(positives, p, prevalence)/n,
prevalence_corr = prev_eff,
probability_null = dbinom(positives, p, prev_eff)/n,
random=sample(n,n,replace = TRUE)/n, #
z.index=ifelse(probit.mode.index<4,probit.z.indices[probit.mode.index],
sample(1:length(z_scores),n,replace=T))) %>%
mutate(
call.each.conc=probit[1+(z.index-1)*571+each.conc*10-(lod-35.9)*10,2]>random,
tests=1 + p * (call.each.conc), # number of tests done (number positive pools + pool size)
tn=0,
tp=1 * (call.each.conc),
fn=1 * (!call.each.conc),
fp=0)
}
}))
}))
}))
}))
}
# apply the model loop to the fake correlated data
#allfirst.poolct <- rbindlist(lapply(1:length(above.lod), experiment_loop, sim_min = 0.1, sim_max = 0.2, ct_dat = ct_fake_input))
allfirst.poolct <- rbindlist(lapply(3, experiment_loop, ct_dat = ct_fake_input)) # just for 15% Ct > LoD
# Model loop, vary % above each LOD (simplified from original code)
# source("MCMC_sim.R")
# contains calc_probs() fxn. This SIMULATES the number of positives per pool
source("compute_probas.R")
# contains exact_probs fxn. This CALCULATES the number of positives per pool
pool.max = 20
# if you want to use the simulated probabilities that don't account for community infections (network only)
# use this to get model_output_corr_indiv_.1.2.csv from experiment_loop()
# sim_probs_.1.2 <- read.csv("sim_probs_.1.2.csv")
# sim_probs_.1.2[,1] <- NULL
experiment_loop <- function(above, ct_dat, sim_min, sim_max){
ct_set <- pmax(5,subset(ct_dat + mat2[above,3], ct_dat + mat2[above,3]<45))
# (1) generate additional set of C_t values with above = 5-30% of values above each LoD; subset to only valid Ct values (<45)
# (2) ct_set is the parallel maximum of the C_t values from (1) vs. 5 (why 5??)
threshold.ct <- c(sample(ct_set, n, replace=T)) # sample ct_set with replacement
rbindlist(lapply(1:pool.max, function (p) { # pool size
rbindlist(lapply(c(0.0001,0.001,0.005, 0.01, 0.03, 0.05, 0.1, 0.2), function(prevalence) { # prevalence ("proportion positive tests")
# sim_probs <- calc_probs_all_homogeneous(p, prevalence, min=sim_min, max=sim_max, B=1000)
# specify range of correlation to sample from (min, max)
rbindlist(lapply(c(0, 0.01, 0.05, 0.1, 0.2, 0.3, 0.4, 0.5,  0.7), function(tau) {
sim_probs <- compute_probas(p, prevalence, tau)
n_eff <- compute_neff(p, p, prevalence, tau)
prev_eff <- compute_p(p, prevalence, tau)
#sim_subset_probs <- compute_probas_subset(50, p, prevalence, tau)
# sim_probs <- exact_probs(group_size = 1, tau = .2, pop_prev = .1)
rbindlist(lapply(0:p, function(positives) { # number of positives
#print(positives)
if (positives == 0) {
# prevalence_corr <-  1-((1-prevalence)*(1-(prevalence*tau))^(p-1))
data.frame(limit=lod, pool=p, pos=positives, prevalence=prevalence,
above.llod = above.lod[above],
concentration=0,
tau = tau,
n_eff = n_eff,
probability = sim_probs[which(sim_probs$n == 0), "p"],
#probability_subset1 =  sim_subset_probs[which(sim_subset_probs$n == 0), "p"],
probability_null_test = dbinom(positives, p, prevalence), # uncorrected prevalence
prevalence_corr =prev_eff,
probability_null = dbinom(positives, p, prev_eff), # prevalence corrected for correlated individuals
random=0, z.index=0,
call.each.conc=FALSE, tests=1, tn=1,tp=0,fn=0,fp=0)
}
else {
dat <- matrix(sample(threshold.ct, positives * n, replace=T), nrow=positives)
# sample data uniformly at random
# n samples of positives, rearrange into a matrix of positive rows, n columns
each.conc = -log2(colSums(2^-dat)/p)+ifelse(dilution.vary.index==1,0,
rnorm(mean=0,sd=1.1,n=ncol(dat)))
# sd of 1.1 reflects confidence interval for deviation from perfect log2 dilution in assays
# calculation dilution based on number of positives (colSum) in total pool size (p)
data.frame(
limit=lod,
pool=p,
pos=positives,
prevalence=prevalence,
above.llod=above.lod[above],
concentration=each.conc,
tau = tau,
n_eff = n_eff,
# probability = (1-(1-prevalence)^p)*sim_probs_.1.2[positives, (p-1)], # using network only infection probs
probability = sim_probs[which(sim_probs$n == positives), "p"]/n,
#probability_subset1 =  sim_subset_probs[which(sim_subset_probs$n == positives), "p"]/n,
probability_null_test = dbinom(positives, p, prevalence)/n,
prevalence_corr = prev_eff,
probability_null = dbinom(positives, p, prev_eff)/n,
random=sample(n,n,replace = TRUE)/n, #
z.index=ifelse(probit.mode.index<4,probit.z.indices[probit.mode.index],
sample(1:length(z_scores),n,replace=T))) %>%
mutate(
call.each.conc=probit[1+(z.index-1)*571+each.conc*10-(lod-35.9)*10,2]>random,
tests=1 + p * (call.each.conc), # number of tests done (number positive pools + pool size)
tn=0,
tp=1 * (call.each.conc),
fn=1 * (!call.each.conc),
fp=0)
}
}))
}))
}))
}))
}
# apply the model loop to the fake correlated data
#allfirst.poolct <- rbindlist(lapply(1:length(above.lod), experiment_loop, sim_min = 0.1, sim_max = 0.2, ct_dat = ct_fake_input))
allfirst.poolct <- rbindlist(lapply(3, experiment_loop, ct_dat = ct_fake_input)) # just for 15% Ct > LoD
